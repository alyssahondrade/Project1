





# Import libraries
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import scipy.stats as stats
import hvplot.pandas

# Import API keys
from alyssa_config import spoonacular_key, rapidapi_key, geoapify_key
# from lakna_config import spoonacular_key

# Import functions notebook
%run functions.ipynb





# Import the FOOD.COM datasets as DataFrames.
food_df = pd.read_csv('Resources/RAW_recipes.csv')
interactions_df = pd.read_csv('Resources/RAW_interactions.csv')





# Display the DataFrame
food_df.head()


# Display the DataFrame
interactions_df.head()


# Get the DataFrame dimensions
interactions_shape = interactions_df.shape
food_shape = food_df.shape

# Print findings
hash = f'{8*"#"}'
print(f'{hash} Shape {hash}')
print(f"food_df: {food_shape}")
print(f"interactions_df: {interactions_shape}")

# Get the columns
print(f'\n{hash} Columns {hash}')
print(f'food_df: {food_df.columns}')
print(f'interactions_df: {interactions_df.columns}')

# Get the datatypes
print(f'\n{hash} Data Types {hash}')
print(f'food_df: {food_df.dtypes}')
print(f'\ninteractions_df: {interactions_df.dtypes}')





# Check how many recipes have ratings
unique_ratings = len(interactions_df['recipe_id'].unique())

# Check how many recipes have a '0' rating
zero_rating_df = interactions_df.loc[interactions_df['rating'] == 0]['recipe_id'].unique()

# Drop the rows with a '0' rating
nonzero_df = interactions_df.loc[interactions_df['rating'] != 0]
nonzero_shape = nonzero_df.shape

# Print findings
print(f'Recipes with ratings: {unique_ratings} out of {food_shape[0]}')
print(f'Recipes with a "0" rating: {zero_rating_df.shape[0]}')
print(f'Updated shape: {nonzero_shape}')


# Create a DataFrame with the average ratings per recipe ID
food_ratings = nonzero_df.groupby('recipe_id')['rating'].mean().reset_index()

# Display the DataFrame
food_ratings.head()


# Rename the recipe ID column for merging with food_df
food_ratings = food_ratings.rename(columns={'recipe_id': 'id'})

# Merge the datasets and display updated DataFrame
merged_food = pd.merge(food_df, food_ratings, on='id')

# Confirm the row dimensions, to ensure correct merge
merged_shape = merged_food.shape
print(f'food_ratings rows: {food_ratings.shape[0]}')
print(f'merged_food rows: {merged_shape[0]}')

# Display the DataFrame
merged_food.head()


# Create a histogram of the ratings
merged_food['rating'].hist(bins=20)
plt.title("Food.com Recipes")
plt.xlabel("Rating")
plt.ylabel("Number of Recipes")
plt.show()





# Determine whether there are duplicate recipes by ID
dup_id = len(merged_food['id'].unique())
print(f'Unique recipe IDs: {dup_id} of {merged_shape[0]}')

# Determine whether there are duplicate recipes by name
dup_name = len(merged_food['name'].unique())
print(f'Unique recipe names: {dup_name} of {merged_shape[0]}')

# Get the duplicate names - remove to simplify the dataset
duplicate_names = merged_food.loc[merged_food.duplicated(['name'])]
dupname_shape = duplicate_names.shape
print(f'Number of duplicate rows to remove: {dupname_shape[0]}\n')

# Create a new DataFrame without the duplicates
updated_food = merged_food.loc[~merged_food.duplicated(['name'])].copy()
updated_shape = updated_food.shape
print(f'updated_food: {updated_shape}')

# Display the DataFrame
updated_food.head()





# Identify how to isolate each tag
tag_string = updated_food['tags'][0].strip("[]")
tag_string = tag_string.split(', ')
tag_string[0].strip("'")

# Get a list of unique tags
unique_tags = []
for string in updated_food['tags']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_tags:
            unique_tags.append(tag)

# Display the tags alphabetically
print(f"Number of unique tags: {len(unique_tags)}")
sorted(unique_tags)





# Check the tags for meal types
meal_types = ["breakfast", "lunch", "dinner"]
match_meals = tag_check(meal_types)

# Print the results
print(f'Tags: {match_meals[0]}')
print(f'Not tags: {match_meals[1]}')
print(f'Alternative tags: {match_meals[2]}\n')

# Update the meal type list
meal_types[2] = "dinner-party"
print(f'Updated meal_types: {meal_types}')


# Get a count of each meal type
meal_dict = parse_tags(meal_types, 'tags', updated_food, 'id')

# Get the list of all recipes with one meal type
combined_meals = []
for key in meal_dict:
    print(f"{key}: {meal_dict[key]['count']}")
    if (key != 'multiple'):
        combined_meals.append(meal_dict[key]['id_list'])

# Flatten combined_meals
flat_meals = [index for meal_list in combined_meals for index in meal_list]

# Remove multiples to get a list of all recipes with one meal type
unique_meals = [meal for meal in flat_meals if meal not in meal_dict['multiple']['id_list']]

# Print results
print(f'\nTotal recipes with one meal type: {len(unique_meals)}')


# Reduce the dataset to contain recipes with only one meal type
reduced_food = updated_food.loc[updated_food['id'].isin(unique_meals)].copy()
reduced_shape = reduced_food.shape

# Create a new column with the meal type
for key in meal_dict:
    for id in meal_dict[key]['id_list']:
        if (key == "dinner-party"):
            key = "dinner"
        reduced_food.loc[reduced_food['id'] == id, 'meal_type'] = key

# Display the DataFrame
print(f'reduced_food: {reduced_shape}')
reduced_food.head()





# Convert Spoonacular supported cuisines to a list
# input_string = input("List to pass: ")
input_string = """African British Cajun Caribbean Chinese
    Eastern European French German Greek Indian Irish Italian
    Japanese Jewish Korean Latin American Mediterranean Mexican Middle Eastern
    Nordic Southern Spanish Thai Vietnamese"""

spoonacular_cuisines = input_string.split(' ')
spoonacular_cuisines = [word.lower() for word in spoonacular_cuisines]
spoonacular_cuisines # DOES NOT ACCOUNT FOR DOUBLE WORD.

# List comprehension to remove double words
double_words = ['eastern', 'european', 'latin', 'american', 'middle']
[spoonacular_cuisines.remove(word) for word in double_words]

# Return the two-word cuisines
spoonacular_cuisines += ['eastern european', 'latin american', 'middle eastern']
spoonacular_cuisines = sorted(spoonacular_cuisines)


# Check the tags for spoonacular cuisines
cuisine_match = tag_check(spoonacular_cuisines)[0]
print(f'Spoonacular Cuisines: {len(spoonacular_cuisines)}\n{spoonacular_cuisines}\n')
print(f'Matched cuisines: {len(cuisine_match)}\n{cuisine_match}')


# Get a count of each cuisine
cuisine_dict = parse_tags(cuisine_match, 'tags', reduced_food, 'id')

# Get the list of all recipes with one meal type
combined_cuisines = []
for key in cuisine_dict:
    print(f"{key}: {cuisine_dict[key]['count']}")
    if (key != 'multiple'):
        combined_cuisines.append(cuisine_dict[key]['id_list'])

# Flatten combined_meals to get a list of unique recipes with one meal type
flat_cuisine = list(set([index for cuisine_list in combined_cuisines for index in cuisine_list]))

# Remove multiples to get a list of all recipes with one cuisine type
unique_cuisine = [cuisine for cuisine in flat_cuisine if cuisine not in cuisine_dict['multiple']['id_list']]
print(f'\nTotal recipes with one cuisine type: {len(unique_cuisine)} of {reduced_food.shape[0]}')


# Reduce the dataset to contain recipes with only one cuisine
one_cuisine = reduced_food.loc[reduced_food['id'].isin(unique_cuisine)].copy()

# Create a new column with the cuisine
key_list = []
for key in cuisine_dict:
    if key != "multiple":
        key_list.append(key)
        for id in cuisine_dict[key]['id_list']:
            one_cuisine.loc[one_cuisine['id'] == id, 'cuisine'] = key

# Confirm unique cuisines
print(f"Unique cuisines:\n {one_cuisine['cuisine'].unique()}")

# Display the DataFrame
print(f'\none_cuisine: {one_cuisine.shape}')
one_cuisine.head()





# Identify how to split the nutrition string and convert to float
test_string = one_cuisine['nutrition']

# Remove the square brackets
# test_string = test_string[1].strip("[]")
test_string = test_string[63].strip("[]")

# Split the string to a list
test_string = test_string.split(", ")

# Cast values to float
test_string = [float(value) for value in test_string]
test_string


# Identify how to convert PDV to nutrient quantity
conversion_ref = [65, 50, 2.4, 50, 20, 300]

# Remove 'Calories' from the test string
test_pdv = test_string[1:]

# Convert from PDV to absolute values
abs_values = []
for ref in range(len(conversion_ref)):
    abs_values.append(test_pdv[ref] * conversion_ref[ref] / 100)
abs_values


one_cuisine.head()


# Parse each value in the `nutrition` column
for df_idx, row in one_cuisine.iterrows():
    # Strip and split the string to a list
    values_list = row['nutrition'].strip("[]").split(", ")

    # Allocate each nutritional value to the correct column
    for idx, value in enumerate(values_list):
        if (idx == 0):
            one_cuisine.loc[df_idx, 'Calories'] = float(value)
        elif (idx == 1):
            one_cuisine.loc[df_idx, 'Total Fat (PDV)'] = float(value)
        elif (idx == 2):
            one_cuisine.loc[df_idx, 'Sugar (PDV)'] = float(value)
        elif (idx == 3):
            one_cuisine.loc[df_idx, 'Sodium (PDV)'] = float(value)
        elif (idx == 4):
            one_cuisine.loc[df_idx, 'Protein (PDV)'] = float(value)
        elif (idx == 5):
            one_cuisine.loc[df_idx, 'Saturated Fat (PDV)'] = float(value)
        elif (idx == 6):
            one_cuisine.loc[df_idx, 'Carbohydrates (PDV)'] = float(value)

# Display the DataFrame
one_cuisine.head()


# Convert PDV nutritional values to absolute values (in grams)
merged_subset = one_cuisine[[
    'Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

for df_idx, row in merged_subset.iterrows():
    for col_idx in range(len(row)):
        result = row[col_idx] * conversion_ref[col_idx] / 100
        if (col_idx == 0):
            one_cuisine.loc[df_idx, 'total_fat_g'] = result
        elif (col_idx == 1):
            one_cuisine.loc[df_idx, 'sugar_g'] = result
        elif (col_idx == 2):
            one_cuisine.loc[df_idx, 'sodium_g'] = result
        elif (col_idx == 3):
            one_cuisine.loc[df_idx, 'protein_g'] = result
        elif (col_idx == 4):
            one_cuisine.loc[df_idx, 'sat_fat_g'] = result
        elif (col_idx == 5):
            one_cuisine.loc[df_idx, 'carbs_g'] = result

# Display the DataFrame
print(f'one_cuisine: {one_cuisine.shape}')
one_cuisine.head()





# Iterate over the DataFrame rows to calculate the WW Smart Points
for df_idx, row in one_cuisine.iterrows():
    calories = row.Calories
    sat_fat = row.sat_fat_g
    sugar = row.sugar_g
    protein = row.protein_g

    # Append the result to the DataFrame
    one_cuisine.loc[df_idx, 'wws_points'] = round((calories * 0.0305) + (sat_fat * 0.275) + (sugar * 1.2) - (protein * 0.98), 0)

# Convert wws_points to integers and display the results
one_cuisine = one_cuisine.astype({'wws_points':'int64'})
one_cuisine.head()








# Check DataFrame datatypes
one_cuisine.dtypes


# Get a list of columns of type int64 and float64, excluding 'id' and 'contributor_id'
calc_cols = one_cuisine.select_dtypes(include=['int64', 'float64']).drop(columns=['id', 'contributor_id'])

# Get the descriptive statistics
calc_stats = calc_cols.describe().T.drop(columns=['count'])

# Calculate the IQR and bounds, then display the descriptive statistics
print(f'Columns with values above "max": {len(add_iqr(calc_stats))}\n{add_iqr(calc_stats)}')
calc_stats = calc_stats.T
calc_stats








# Get upper_bounds for PDV columns
pdv_cols = ['Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']
upper_pdv = calc_stats.loc['upper_bounds', pdv_cols]

# Create a dictionary to hold the count and IDs
pdv_dict = {}
for col in upper_pdv.index:
    pdv_dict[col] = dict(count = 0, id_list = [])

# Loop through upper_pdv to get the count and IDs
for idx, value in enumerate(upper_pdv):
    col_name = upper_pdv.index[idx]
    pdv_dict[col_name]['count'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id'].count()
    pdv_dict[col_name]['id_list'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id']

# Get the list of all recipes which exceeds upper bounds for PDV
combined_pdv = []
for key in pdv_dict:
    print(f"{key}: {pdv_dict[key]['count']}")
    combined_pdv.append(pdv_dict[key]['id_list'])

# Flatten combined_pdv
flat_pdv = list(set([index for pdv_list in combined_pdv for index in pdv_list]))
print(f'Total recipes which exceed upper bounds for PDV: {len(flat_pdv)}')


# Explore values outlier values
outside_pdv = one_cuisine.loc[one_cuisine['id'].isin(flat_pdv)]
outside_pdv.sort_values(by='wws_points', ascending=False).head()

# Although PDV correlates to a single serving, the outliers identified are not reasonable.
# This also corresponds to very high WW Smart Points, which is not realistic.
# This could be attributed to errors in inputing serving sizes for the recipe.


# Remove the outliers and create a new DataFrame: within_pdv
within_pdv = one_cuisine.loc[~one_cuisine['id'].isin(flat_pdv)].copy()
print(f'within_pdv: {within_pdv.shape}')
within_pdv.head()


# Check the desciptive statistics for the cleaner DataFrame
within_pdv[pdv_cols].describe()


# Display the top 10 recipes with the highest WW Smart Points (i.e. very unhealthy)
within_pdv.sort_values(by='wws_points', ascending=False).head(10)





# Investigate "unhealthy recipes", with PDV well outside the range
plot_rows = int(len(pdv_cols)/2)
plot_cols = 2
fig, axs = plt.subplots(plot_rows, plot_cols, figsize=(10, 10))

# Loop through 
for idx, ax in enumerate(axs.reshape(-1)):
    # Get the x- and y-values
    x_values = outside_pdv['wws_points']
    y_values = outside_pdv[pdv_cols[idx]]

    # Create each plot
    ax.scatter(x_values, y_values)
    ax.title.set_text(f'Very High PDV: {pdv_cols[idx]}')
    ax.set_xlabel("WW Smart Points")
    ax.set_ylabel(pdv_cols[idx])

plt.tight_layout(pad=3)








# Identify recipes with '0' minutes cooking time
zeromin_recipes = within_pdv.loc[within_pdv['minutes'] == 0]
print(f'zeromin_recipes: {zeromin_recipes.shape}')
zeromin_recipes.sort_values(by='minutes', ascending=False)


# Get the upper bounds value from the 'minutes' column
minutes_upper = calc_stats.loc['upper_bounds', 'minutes']

# Identify recipes with very long cooking times
long_recipes = within_pdv.loc[within_pdv['minutes'] > minutes_upper]
print(f'long_recipes: {long_recipes.shape}')
long_recipes.sort_values(by='minutes', ascending=False)


# Remove recipes greater than the calculated upper bounds AND recipes with '0' minutes cooking time
clean_recipes = within_pdv.loc[(within_pdv['minutes'] < minutes_upper) & (within_pdv['minutes'] > 0)].copy()
print(f'clean_recipes: {clean_recipes.shape}')
clean_recipes.head()


# Check the desciptive statistics for the cleaner DataFrame
clean_recipes['minutes'].describe()





# How many of the long recipes had PDV outliers?
pdv_long = one_cuisine.loc[one_cuisine['id'].isin(flat_pdv)]
pdv_long.sort_values(by='minutes', ascending=False)


pdv_mins = pdv_long.loc[pdv_long['minutes'] > minutes_upper]
print(f'Recipes with PDV outliers: {pdv_long.shape[0]}')
print(f'Long recipes with PDV outliers: {pdv_mins.shape[0]}')


# How many of the long recipes are highly rated?
best_long = long_recipes.loc[long_recipes['rating'] == 5].copy()
best_long['minutes'].describe()








# Get the descriptive statistics summary
clean_descript = clean_recipes['wws_points'].describe()
clean_descript


# Create a histogram of the points
clean_recipes['wws_points'].hist(bins=20)
plt.title("Food.com Recipes - Population")
plt.xlabel("WW Smart Points")
plt.ylabel("Number of Recipes")
plt.show()


# Check for Gaussian distribution - using Shapiro-Wilk test
# Note: Shapiro-Wilk is sensitive to sample size
sample_size = 500
sample_set = clean_recipes['wws_points'].sample(n=sample_size)
sample_set.hist(bins=15)
plt.title(f"Food.com Recipes - Sample (n={sample_size})")
plt.xlabel("Weight Watchers (WW) Smart Points")
plt.ylabel("Number of Recipes")

stats.shapiro(sample_set)
# A very high W-statistic suggests a good fit to a normal distribution.
# A near-zero p-value, for a Shapiro-Wilk test, rejects the null hypothesis, meaning NOT a normal distribution.





# Get clean_recipes['wws_points'] outliers
points_outliers = add_iqr(clean_descript)
points_outliers


# Create a new DataFrame without WW Smart Points outliers
clean_recipes1 = clean_recipes.loc[
    (clean_recipes['wws_points'] < points_outliers.upper_bounds)
    & (clean_recipes['wws_points'] > points_outliers.lower_bounds)]

# Get the descriptive statistics
clean_descript1 = clean_recipes1.describe()
print(f"wws_points outliers removed: {clean_descript1['wws_points']}")
points_outliers1 = add_iqr(clean_descript1['wws_points'])

# Check for Gaussian distribution
sample_set1 = clean_recipes1['wws_points'].sample(n=500)
sample_set1.hist(bins=15)
plt.title(f"Food.com Recipes - Outliers Removed - Sample (n={sample_size})")
plt.xlabel("Weight Watchers (WW) Smart Points")
plt.ylabel("Number of Recipes")

stats.shapiro(sample_set1)
# Removing outliers results in a higher W-statistic, however the p-value is still near-zero.





clean_recipes['ingredients']


# Get a list of unique tags
unique_ingredients = []
for string in clean_recipes['ingredients']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_ingredients:
            unique_ingredients.append(tag)

# Display the tags alphabetically
print(f"Number of unique ingredients: {len(unique_ingredients)}")
sorted_ingredients = sorted(unique_ingredients)
sorted_ingredients


test = parse_tags(sorted_ingredients, 'ingredients', clean_recipes, 'id')


test


breakfast_ingred = clean_recipes.loc[clean_recipes['meal_type'] == "breakfast"]
breakfast_ingred
brekky = parse_tags(sorted_ingredients, 'ingredients', breakfast_ingred, 'id')

max_ingred = 0
top_ingred = []
for idx, key in enumerate(brekky):
    get_count = brekky[key]['count']
    if key != "multiple": # Ignore the 'multiple' list, not applicable
        if get_count > max_ingred:
            top_ingred.append(key)
            max_ingred = get_count
print(f'{top_ingred}, {max_ingred}')
print(breakfast_ingred.shape[0])


for meal in clean_recipes['meal_type'].unique():
    df = clean_recipes.loc[clean_recipes['meal_type'] == meal]
    df_parsed = parse_tags(sorted_ingredients, 'ingredients', df, 'id')

    max_count = 0
    top_count = []
    top_ingredient = []
    for idx, key in enumerate(df_parsed):
        get_count = df_parsed[key]['count']
        # Ignore the 'multiple' list, not applicable
        if (key != 'multiple'):
            if (get_count > max_count):
                top_ingredient.append(key)
                top_count.append(get_count)
                max_count = get_count
    print(f'{meal.title()}: {top_ingredient[-3:]} = {top_count[-3:]} of {df.shape[0]}')





# Drop the `nutrition` column
clean_recipes = clean_recipes.drop(columns=['contributor_id', 'nutrition', 'steps', 'description'])
clean_recipes.head()


# Output results to a csv
clean_recipes.to_csv('Output/clean_recipes.csv', index=False)





# Create a DataFrame of points of interest for the correlation calculation
pdv_cut = clean_recipes[[
    'rating', 'Calories', 'Total Fat (PDV)',
    'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)',
    'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

# Calculate the correlation matrix
corr_matrix = round(pdv_cut.corr(), 2)
corr_matrix

# Create a heatmap based on the results
corr_matrix.style.background_gradient(cmap='Blues')








# Isolate 'breakfast, lunch, dinner' from clean_recipes
meal_df = clean_recipes.groupby(['meal_type']).median(numeric_only=True)
meal_df = meal_df.drop(columns=['id', 'contributor_id'])
meal_df


# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes['wws_points'].describe().min()
max_points = clean_recipes['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points+16, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
points_labels = [f'{points_bins[i]} to {points_bins[i+1]}' for i in range(len(points_bins)-1)]
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins, labels=points_labels)

# Set the index and its name
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges', 'meal_type']).median(numeric_only=True) # originally mean
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)

## RESEARCH HOW TO PLOT MULTI-INDEX DATAFRAME


# Extract data for plotting
x = binned_df.index.get_level_values('WWS Point Ranges')
y = binned_df.index.get_level_values('meal_type')
size = binned_df['protein_g']*10  # Size of the bubbles
colour = binned_df['rating']  # Color of the bubbles

# Create the bubble chart
fig, ax = plt.subplots(figsize=(8, 6))
scatter = ax.scatter(x, y, s=size * 10, c=colour, cmap='viridis', alpha=0.7)

# Add labels and a colorbar
ax.set_xlabel('WWS Points')
plt.xticks(rotation=45)
ax.set_ylabel('Meal Type')
ax.set_title('Meal Types Bubble Chart')
cbar = plt.colorbar(scatter)
cbar.set_label('Rating')

plt.show()


# THOUGHTS: Group by ratings instead? Rather than points? More robust and less iffy that way?
# ANSWER: Tried it, doesn't really work!


updated_types = clean_recipes['meal_type'].unique()
updated_types = ['breakfast', 'lunch', 'dinner']

# Breakfast
breakfast = clean_recipes.loc[clean_recipes['meal_type'] == "breakfast"].copy()
print(f'breakfast: {breakfast.shape}')

# Lunch
lunch = clean_recipes.loc[clean_recipes['meal_type'] == "lunch"].copy()
print(f'lunch: {lunch.shape}')

# Dinner
dinner = clean_recipes.loc[clean_recipes['meal_type'] == "dinner"].copy()
print(f'dinner: {dinner.shape}')


fig, axs = plt.subplots(3, sharex=True, sharey=True)
fig.suptitle("WW Smart Points")
x_col = 'wws_points'
y_col = 'rating'
size_col = 'protein_g'
colour_col = 'sat_fat_g'

df_list = [breakfast, lunch, dinner]

for idx, val in enumerate(df_list):

    # Bin by `wws_points`
    min_points = df_list[idx]['wws_points'].describe().min()
    max_points = df_list[idx]['wws_points'].describe().max()
    
    # Create bins
    points_bins = np.arange(min_points, max_points, 15) # 5 was good with less granular cuisines
    points_labels = [f'{points_bins[i]} to {points_bins[i+1]}' for i in range(len(points_bins)-1)]
    bins_df = pd.cut(df_list[idx]['wws_points'], bins=points_bins, labels=points_labels)
    
    # Set the index and its name
    points_df = df_list[idx].set_index(bins_df)
    points_df.index.name = "WWS Point Ranges"
    
    # Create grouped DataFrame by numeric means
    binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
    binned_df

    # Extract x- and y-values
    x = binned_df[x_col]
    y = binned_df[y_col]
    size = binned_df[size_col]
    colour = binned_df[colour_col]

    scatter_plot = axs[idx].scatter(x, y, s=size*10, alpha=0.3, c=colour)
    axs[2].set_xlabel('WW Smart Points')
    axs[idx].set_ylabel(updated_types[idx].title())
    cbar = plt.colorbar(scatter_plot)
    cbar.set_label(colour_col)

    (slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x, y)
    line_equation = f'y = {round(slope, 2)}x + {round(intercept, 2)}'
    regression_values = slope * x + intercept
    axs[idx].plot(x, regression_values, linestyle='--', c='red')
    if idx == 0:
        x_coord, y_coord = 50, 240
    elif idx == 1:
        x_coord, y_coord = 50, 150
    elif idx == 2:
        x_coord, y_coord = 50, 50
    axs[idx].annotate(line_equation, xy=(x_coord, y_coord), xycoords='figure points',
        fontsize = 12, color='red', weight='bold')

    print(f'{updated_types[idx]} R-value: {round(rvalue**2, 2)}')


# Bin and count to get percentages?
meal_median = []
for idx, val in enumerate(df_list):
    meal_median.append(df_list[idx]['wws_points'].describe()['50%'])
mealmedian_avg = np.mean(meal_median)
mealmedian_avg # Cut off for healthiness

# THIS WOULD JUST MAKE IT 50% healthy and 50% unhealthy???
# Use nutritional values instead, and create percentage comparisons based on how many exceed all values? Is this realistic?


# Create a DataFrame grouped by the means of each cuisine
meal_df = clean_recipes.groupby(['meal_type']).median(numeric_only=True)

# Get percentage of "healthy" recipes per cuisine
# Set the limit as the median 'wws_points'
wws_median = clean_recipes['wws_points'].describe()['50%']

# Get the total number of recipes per cuisine
meal_count = clean_recipes.groupby(['meal_type'])['id'].count()

# Create a DataFrame of recipes that sit above the median 'wws_points'
healthy_count = clean_recipes.loc[clean_recipes['wws_points'] < wws_median]

# Create a groupby DataFrame that counts the number of healthy recipes
healthy_group = healthy_count.groupby(['meal_type'])['id'].count()

# Calculate the percentage value
percent_healthy = []
for idx in range(len(meal_count)):
    percent_healthy.append(int(healthy_group[idx] * 100 / meal_count[idx]))

# Create a DataFrame with calculated percentages, add the average WWS Points as a new column
percent_df = pd.DataFrame(percent_healthy, index=healthy_group.index, columns=['percent_healthy'])
percent_df['ave_wws_points'] = meal_df['wws_points']

# Sort the values and reset index to recover 'cuisine'
percent_df = percent_df.sort_values(by=['ave_wws_points'], ascending=False).reset_index()

# Plot the bar chart
bar_chart = percent_df['ave_wws_points'].plot(
    kind = "bar",
    title = "Percentage of Healthy Recipes per Meal Type - Version 1",
    edgecolor = "white")

# Set the labels
bar_chart.set_xlabel("Meal Types")
bar_chart.set_ylabel("Average WW Smart Points")
cuisine_titles = [word.title() for word in percent_df['meal_type']]
bar_chart.set_xticklabels(
    cuisine_titles,
    rotation_mode='default',
    rotation=0)

# Annotate the bar chart with the percentage values
for index, row in percent_df.iterrows():
    plt.annotate(
        f"{row['percent_healthy']}%", color="green",
        xy=(row.name, meal_df[meal_df.index == row['meal_type']]['wws_points']),
        xytext=(-10,0), textcoords='offset points')


# Create a DataFrame grouped by the means of each cuisine
meal_df = clean_recipes.groupby(['meal_type']).mean(numeric_only=True)

# Get percentage of "healthy" recipes per cuisine
# Set the limit as the median 'wws_points'
wws_median = clean_recipes['wws_points'].describe()['50%']

# Get the total number of recipes per cuisine
meal_count = clean_recipes.groupby(['meal_type'])['id'].count()

# Create a DataFrame of recipes that sit above the median 'wws_points'
healthy_count = clean_recipes.loc[clean_recipes['wws_points'] < wws_median]

# Create a groupby DataFrame that counts the number of healthy recipes
healthy_group = healthy_count.groupby(['meal_type'])['id'].count()

# Calculate the percentage value
percent_healthy = []
for idx in range(len(meal_count)):
    percent_healthy.append(int(healthy_group[idx] * 100 / meal_count[idx]))

# Create a DataFrame with calculated percentages, add the average WWS Points as a new column
percent_df = pd.DataFrame(percent_healthy, index=healthy_group.index, columns=['percent_healthy'])
percent_df['ave_wws_points'] = meal_df['wws_points']

# Sort the values and reset index to recover 'cuisine'
percent_df = percent_df.sort_values(by=['percent_healthy'], ascending=True).reset_index()

# Plot the bar chart
bar_chart = percent_df['ave_wws_points'].plot(
    kind = "bar",
    title = "Percentage of Healthy Recipes per Meal Type - Version 2",
    edgecolor = "white")

# Set the labels
bar_chart.set_xlabel("Meal Types")
bar_chart.set_ylabel("Average WW Smart Points")
cuisine_titles = [word.title() for word in percent_df['meal_type']]
bar_chart.set_xticklabels(
    cuisine_titles,
    rotation_mode='default',
    rotation=0)

# Annotate the bar chart with the percentage values
for index, row in percent_df.iterrows():
    plt.annotate(
        f"{row['percent_healthy']}%", color="green",
        xy=(row.name, meal_df[meal_df.index == row['meal_type']]['wws_points']),
        xytext=(-10,0), textcoords='offset points')





# Create a DataFrame grouped by the means of each cuisine
cuisine_df = clean_recipes.groupby(['cuisine']).median(numeric_only=True)

# Get percentage of "healthy" recipes per cuisine
# Set the limit as the median 'wws_points'
wws_median = clean_recipes['wws_points'].describe()['50%']

# Get the total number of recipes per cuisine
cuisine_count = clean_recipes.groupby(['cuisine'])['id'].count()

# Create a DataFrame of recipes that sit above the median 'wws_points'
healthy_count = clean_recipes.loc[clean_recipes['wws_points'] < wws_median]

# Create a groupby DataFrame that counts the number of healthy recipes
healthy_group = healthy_count.groupby(['cuisine'])['id'].count()

# Calculate the percentage value
percent_healthy = []
for idx in range(len(cuisine_count)):
    percent_healthy.append(int(healthy_group[idx] * 100 / cuisine_count[idx]))

# Create a DataFrame with calculated percentages, add the average WWS Points as a new column
percent_df = pd.DataFrame(percent_healthy, index=healthy_group.index, columns=['percent_healthy'])
percent_df['ave_wws_points'] = cuisine_df['wws_points']

# Sort the values and reset index to recover 'cuisine'
percent_df = percent_df.sort_values(by=['ave_wws_points'], ascending=False).reset_index()

# Plot the bar chart
bar_chart = percent_df['ave_wws_points'].plot(
    kind = "bar",
    title = "Percentage of Healthy Recipes per Cuisine - Version 1",
    figsize = (8,6),
    width = 1,
    linewidth = 10,
    edgecolor = "white")

# Set the labels
bar_chart.set_xlabel("Cuisines")
bar_chart.set_ylabel("Average WW Smart Points")
cuisine_titles = [word.title() for word in percent_df['cuisine']]
bar_chart.set_xticklabels(
    cuisine_titles,
    rotation_mode='default',
    rotation=45,
    ha='right')

# Annotate the bar chart with the percentage values
for index, row in percent_df.iterrows():
    plt.annotate(
        f"{row['percent_healthy']}%", color="green",
        xy=(row.name, cuisine_df[cuisine_df.index == row['cuisine']]['wws_points']),
        xytext=(-10,0), textcoords='offset points')


# Sort the values and reset index to recover 'cuisine'
percent_df = percent_df.sort_values(by=['percent_healthy'], ascending=True).reset_index()

# Plot the bar chart
bar_chart = percent_df['ave_wws_points'].plot(
    kind = "bar",
    title = "Percentage of Healthy Recipes per Cuisine - Version 2",
    figsize = (8,6),
    width = 1,
    linewidth = 10,
    edgecolor = "white")

# Set the labels
bar_chart.set_xlabel("Cuisines")
bar_chart.set_ylabel("Average WW Smart Points")
cuisine_titles = [word.title() for word in percent_df['cuisine']]
bar_chart.set_xticklabels(
    cuisine_titles,
    rotation_mode='default',
    rotation=45,
    ha='right')

# Annotate the bar chart with the percentage values
for index, row in percent_df.iterrows():
    plt.annotate(
        f"{row['percent_healthy']}%", color="green",
        xy=(row.name, cuisine_df[cuisine_df.index == row['cuisine']]['wws_points']),
        xytext=(-10,0), textcoords='offset points')


# Version 3 - This just looks messy, since combining two different y-axes!
percent_df.plot(kind="bar")
plt.title("Percentage of Healthy Recipes per Cuisine - Version 3")
plt.show()


# Create a DataFrame grouped by the means of each cuisine
cuisine_df = clean_recipes.groupby(['cuisine']).median(numeric_only=True)

# Get percentage of "healthy" recipes per cuisine
# Set the limit as the median 'wws_points'
wws_median = clean_recipes['rating'].describe()['50%']

# Get the total number of recipes per cuisine
cuisine_count = clean_recipes.groupby(['cuisine'])['id'].count()

# Create a DataFrame of recipes that sit above the median 'wws_points'
healthy_count = clean_recipes.loc[clean_recipes['rating'] < wws_median]

# Create a groupby DataFrame that counts the number of healthy recipes
healthy_group = healthy_count.groupby(['cuisine'])['id'].count()

# Calculate the percentage value
percent_healthy = []
for idx in range(len(cuisine_count)):
    percent_healthy.append(int(healthy_group[idx] * 100 / cuisine_count[idx]))

# Create a DataFrame with calculated percentages, add the average WWS Points as a new column
percent_df = pd.DataFrame(percent_healthy, index=healthy_group.index, columns=['percent_healthy'])
percent_df['ave_rating'] = cuisine_df['rating']

# Sort the values and reset index to recover 'cuisine'
percent_df = percent_df.sort_values(by=['ave_rating'], ascending=False).reset_index()

# Plot the bar chart
bar_chart = percent_df['ave_rating'].plot(
    kind = "bar",
    title = "Percentage of Healthy Recipes per Cuisine",
    figsize = (8,6),
    width = 1,
    linewidth = 10,
    edgecolor = "white")

# Set the labels
bar_chart.set_xlabel("Cuisines")
bar_chart.set_ylabel("Average Rating")
cuisine_titles = [word.title() for word in percent_df['cuisine']]
bar_chart.set_xticklabels(
    cuisine_titles,
    rotation_mode='default',
    rotation=45,
    ha='right')

# Annotate the bar chart with the percentage values
for index, row in percent_df.iterrows():
    plt.annotate(
        f"{row['percent_healthy']}%", color="green",
        xy=(row.name, cuisine_df[cuisine_df.index == row['cuisine']]['rating']),
        xytext=(-10,0), textcoords='offset points')


# Get the latitudes and longitudes for each cuisine region
regions = ['Congo', 'Louisiana, USA', 'Caribbean', 'France', 'German', 'Greece', 'India',
       'Ireland', 'Japan', 'Korea', 'Mexico', 'Spain', 'Thailand', 'Vietnam']

api_response = request_latlong(regions)
lats = api_response[0]
longs = api_response[1]


cuisine_df['latitude'] = lats
cuisine_df['longitude'] = longs
cuisine_df

cuisine_df.to_csv('Output/cuisine_dataframe.csv', index=False)


cuisine_map = cuisine_df.hvplot.points(
    'longitude',
    'latitude',
    color = 'rating',
    # cmap = ['purple', 'red'],
    size = 'wws_points',
    scale = 30,
    alpha = 0.3,
    geo = True,
    tiles = "CartoLight",
    frame_width = 1000,
    frame_height = 800)
cuisine_map





# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes['wws_points'].describe().min()
max_points = clean_recipes['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 8) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)

# Set the index and its name
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
# display(binned_df)

# Set the x- and y-values for the plot
x_val = binned_df['wws_points']
y_val = binned_df['rating']

fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)


# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes['wws_points'].describe().min()
max_points = clean_recipes['wws_points'].describe().max()

# Create bins
for num in range(1,30):
    points_bins = np.arange(min_points, max_points, num) # CHANGE BETWEEN 10 and 16, and look at PLOT!
    print(num)
    bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)
    
    # Set the index and its name
    points_df = clean_recipes.set_index(bins_df)
    points_df.index.name = "WWS Point Ranges"
    
    # Create grouped DataFrame by numeric means
    binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
    # display(binned_df)
    
    # Set the x- and y-values for the plot
    x_val = binned_df['wws_points']
    y_val = binned_df['rating']
    
    fig, ax = plt.subplots()
    ax.scatter(x_val, y_val, s=binned_df['protein_g']*20, alpha=0.3)
    ax.set_xlabel("WW Smart Points")
    ax.set_ylabel("Rating")
    # ax.plot(x_val, y_val)
    linreg_plot(ax, x_val, y_val, 100, 100)





# Get the DataFrame for 5-star ratings
top_rated = clean_recipes.loc[clean_recipes['rating'] >= 5].copy()
print(f'top_rated: {top_rated.shape}')


x_values = top_rated['wws_points'].unique()
y_values = top_rated['wws_points'].value_counts()

plt.bar(x_values, y_values)
plt.title("WW Smart Points Distribution for 5-star recipes")
plt.xlabel("WW Smart Points")
plt.ylabel("Number of Recipes")
plt.show()


# Get the DataFrame for all other ratings
bottom_rated = clean_recipes.loc[clean_recipes['rating'] < 5].copy()
print(f'bottom_rated: {bottom_rated.shape}')


# Bin by `wws_points` then groupby each meal type
min_points = bottom_rated['wws_points'].describe().min()
max_points = bottom_rated['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(bottom_rated['wws_points'], bins=points_bins)

# Set the index and its name
points_df = bottom_rated.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['protein_g']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['sugar_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Protein (g)")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 150, 200)


# Bin by `wws_points` then groupby each meal type
min_points = top_rated['wws_points'].describe().min()
max_points = top_rated['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(top_rated['wws_points'], bins=points_bins)

# Set the index and its name
points_df = top_rated.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['protein_g']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['sugar_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("sat_fat_g")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)





low_points = clean_recipes.loc[clean_recipes['wws_points'] < -30].copy()
low_points.describe()
low_points.shape


# Bin by `wws_points` then groupby each meal type
min_points = low_points['wws_points'].describe().min()
max_points = low_points['wws_points'].describe().max()

x_val = low_points['wws_points']
y_val = low_points['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Protein (g)")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 150, 200)


low_points['id']


low_points


# Remove outliers?
clean_recipes1 = clean_recipes.loc[~clean_recipes['id'].isin(low_points['id'])]

# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes1['wws_points'].describe().min()
max_points = clean_recipes1['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(clean_recipes1['wws_points'], bins=points_bins)

# Set the index and its name
points_df = clean_recipes1.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).median(numeric_only=True) # originally mean
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*15, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)














# Uncomment below to run [API REQUEST - Complex Search]
# complex_search()

# Import and display complex search results
recipes_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_0.csv')
print(f'recipes_df: {recipes_df.shape}')
recipes_df.head()

# Uncomment below to run [API REQUEST - Recipe Information]
# recipe_info('Resources/02_raw_data/info_master_0.csv')

# Uncomment below to run [API ACTIVE - Check Response]
# check_response()


# Import and display recipe information results
info_df = pd.read_csv(f'Resources/02_raw_data/info_master_0.csv')

# Display the DataFrame and its columns
print(info_df.columns)
info_df.head()


# Parse the API response - all relevant columns, and account for different units

# Uncomment below to run [API ACTIVE - Parse Response]
# parse_response('Resources/03_simplified_data/initial_nutrition_0.csv')

# Import and display recipe information results
simple_df = pd.read_csv(f'Resources/03_simplified_data/initial_nutrition_0.csv')

# Display the DataFrame and its columns
print(simple_df.columns)
simple_df.head()


# Check the units columns
units_columns = ['Calories (Unit)', 'Saturated Fat (Unit)', 'Sugar (Unit)', 'Sodium (Unit)', 'Protein (Unit)']
for col in units_columns:
    check = simple_df[col].unique()
    print(f"Unit check: {col} {check}")

# Account for the differences when calculating points





# Uncomment below to run [API REQUEST - Random Recipe]
# random_recipe('Resources/01_recipe_IDs/initial_recipes_4.csv')

# Import and display random recipes results
random_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_4.csv')
random_df.head()


# # Uncomment block to run [API REQUEST - Recipe Information]
# recipe_info('Resources/02_raw_data/info_master_4.csv')

# # Uncomment block to run [API ACTIVE - Parse Response]
# parse_response('Resources/03_simplified_data/initial_nutrition_4.csv')





# # Uncomment block to run [API REQUEST - Random Recipe and Nutrition by ID]
# recipe_info('Resources/02_raw_data/info_master_5.csv')

# # Uncomment block to run [API REQUEST - Nutrition by ID]
# nutrition_id('Resources/02_raw_data/info_master_5.csv')

# # Uncomment block to parse the response (added carbohydrates)
# parse_metadata(
#     'Resources/01_recipe_IDs/initial_recipes_5.csv',
#     'Resources/03_simplified_data/initial_nutrition_5.csv')





# Uncomment the line below to run Method 1
# spoonacular_v1(type=2)

# Uncomment the line below to run Method 2
# spoonacular_v2()





# Get list of subdirectories in the Resources folder
from pathlib import Path
subdir_list = []
for path in Path('Resources').iterdir():
    # Ignore `.ipynb_checkpoints`
    if (path == Path('Resources/.ipynb_checkpoints')):
        continue
    elif path.is_dir():
        subdir_list.append(path)

# Loop over folders without simplified data
raw_ids = []
raw_files = []
for dir in subdir_list[0:2]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory, ignore `.ipynb_checkpoints`
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints'):
            filename = f'{dir}/{file.name}'
            csv_df = pd.read_csv(filename)
            try:
                raw_ids.append(csv_df['id'])
                raw_files.append(filename)
            except:
                # If no 'id' column, move on to the next csv
                continue

# Get a unique list of recipe IDs
raw_list = [id for row in raw_ids for id in row]
unique_raw = list(set(raw_list))

# Remove NaN values from unique_raw
clean_raw = []
for value in unique_raw:
    try:
        clean_raw.append(int(value))
    except:
        # Value cannot be cast to int
        continue
print(f'Unique out of raw data: {len(clean_raw)} of {len(raw_list)}')

# Loop over folders with simplified data
simple_ids = []
simple_files = []
for dir in subdir_list[2:]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory, ignore `.ipynb_checkpoints`
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints') & (file.name.split('.')[-1] == 'csv'):
                filename = f'{dir}/{file.name}'
                csv_df = pd.read_csv(filename)
                try:
                    try:
                        simple_ids.append(csv_df['id'])
                    except:
                        simple_ids.append(csv_df['ID'])
                    simple_files.append(filename)
                except:
                    # If no 'id' column, move on to the next csv
                    continue

# Get a unique list of recipe IDs
simple_list = [id for row in simple_ids for id in row]
unique_simple = list(set(simple_list))
print(f'Unique out of simplified data: {len(unique_simple)} of {len(simple_list)}')

# Identify the intersection between the unique raw and unique simplified data
intersect_ids = []
missing_ids = set(clean_raw).difference(set(unique_simple))
if set(clean_raw) & set(unique_simple):
    intersect_ids.append(set(clean_raw) & set(unique_simple))
print(f'Total intersection: {len(intersect_ids[0])}')
print(f'Total missing: {len(missing_ids)}')


# If there are IDs with missing information, get the data
if len(missing_ids) > 0:
    spoonacular_v3(missing_ids)





# Import one file from 03_simplified_data as a reference
simple_ref = pd.read_csv('Resources/03_simplified_data/initial_nutrition_7.csv').columns
simple_ref


# Import one file from 04_complex_test as a reference
complex_ref = pd.read_csv('Resources/04_complex_test/simplified_data_0.csv').columns
complex_ref


# Read each simplified data file and create a DataFrame
df_list = []
for file in simple_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
simplified_df = pd.concat(df_list, ignore_index=True)

# Identify duplicate IDs
duplicate_IDs = simplified_df.loc[simplified_df.duplicated(['ID'])]
print(f'Number of duplicate rows to remove: {duplicate_IDs.shape[0]}')

# Create a new DataFrame without the duplicates
simplified_df = simplified_df.loc[~simplified_df.duplicated(['ID'])].copy()

# Display the DataFrame, columns, and shape
print(f'simplified_df: {simplified_df.shape}')
print(simplified_df.columns)
simplified_df.head()


# Read each raw data file and create a DataFrame
df_list = []
for file in raw_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
raw_df = pd.concat(df_list, ignore_index=True)

# Drop rows with missing IDs
raw_df = raw_df.loc[~raw_df['id'].isna()]

# Reduce DataFrame to unique IDs
raw_df = raw_df.loc[~raw_df.duplicated(['id'])].copy()

# Display the DataFrame, columns, and shape
print(f'raw_df: {raw_df.shape}')
print(raw_df.columns)
raw_df.head()


# Minimum columns required
min_cols = ['ID', 'Name', 'Likes', 'Meal Type', 'Cuisines', 'N_ingredients',
       'WW Smart Points', 'Cooking Minutes', 'Calories (Amount)', 'Calories (Unit)',
        'Calories (% of Daily Needs)', 'Saturated Fat (Amount)', 'Saturated Fat (Unit)',
       'Saturated Fat (% of Daily Needs)', 'Sugar (Amount)', 'Sugar (Unit)',
       'Sugar (% of Daily Needs)', 'Sodium (Amount)', 'Sodium (Unit)',
       'Sodium (% of Daily Needs)', 'Protein (Amount)', 'Protein (Unit)',
       'Protein (% of Daily Needs)', 'Carbs (Amount)', 'Carbs (Unit)',
       'Carbs (% of Daily Needs)']

# Identify columns with missing data
missing_cols = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')





# Get the list of recipe IDs with missing data
missing_recipes = check_na['ID']

# Check if the missing data have rows in raw_df
find_missing = raw_df.loc[raw_df['id'].isin(missing_recipes)]

# Check if there is a nutrition column for the missing data
find_nutrition = raw_df.loc[~raw_df['nutrition'].isna()]
print(f'Missing recipes in raw_df with a nutrition column: {len(find_nutrition)}')


# Test get_nutrition() function
test_nutrition = get_nutrition(raw_df['nutrition'][0], 'Carbohydrates')
test_nutrition


# Populate simplified_df with the missing data
for df_idx, row in find_nutrition.iterrows():
    result = get_nutrition(row['nutrition'], 'Carbohydrates')
    recipe_id = find_nutrition.loc[df_idx, 'id']
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = result[0]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = result[1]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = result[2]

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')

# Flatten list of missing IDs
missing_list = [id for id_list in missing_ids for id in id_list]


# Identify the recipes with still missing values
still_missing = simplified_df.loc[simplified_df['ID'].isin(missing_list)]
check_column = still_missing.loc[still_missing['Carbohydrates'].isna()]

# Display the DataFrame
print(f'Carbohydrates column with missing values: {len(check_column)}')
still_missing.head() # Data is in 'Carbohydrates' column


# Drop null values from the carbohydrates column
still_missing = still_missing.dropna(subset='Carbohydrates', how="any")

# Add carbohydrates columns
for df_idx, row in still_missing.iterrows():
    carbs = row['Carbohydrates']
    recipe_id = still_missing.loc[df_idx, 'ID']
    # Check if the units are consistent (i.e. no 'mg')
    if "m" in carbs:
        print("Inconsistent units")
    else:
        amount = float(carbs[:-1])
        units = carbs[-1]
        pdv = round(100 * amount / conversion_ref[5], 2)

        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = amount
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = units
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = pdv

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')

# Flatten list of missing IDs
missing_list = [id for id_list in missing_ids for id in id_list]


# Identify the recipes with still missing values
still_missing = simplified_df.loc[simplified_df['ID'].isin(missing_list)]
check_column = still_missing.loc[still_missing['Cooking Minutes'].isna()]

# Display the DataFrame
print(f'Cooking Minutes column with missing values: {len(check_column)}')
still_missing.head() # Data is in 'readyInMinutes' column

# Drop the 'Prep Minutes' column
simplified_df = simplified_df.drop(columns=['Prep Minutes'])
simplified_df.columns


# Add cooking minutes column
for df_idx, row in still_missing.iterrows():
    recipe_id = still_missing.loc[df_idx, 'ID']
    mins = raw_df.loc[raw_df['id'] == recipe_id, 'readyInMinutes']
    simplified_df['Cooking Minutes'] = float(mins)

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')





simplified_df['Meal Type']


updated_types
meal_dict = parse_tags(updated_types, 'Meal Type', simplified_df, 'ID')
all_meals = []
for key in meal_dict:
    print(f'{key}: {meal_dict[key]["count"]}')
    if key != "multiple":
        all_meals.append(meal_dict[key]['id_list'])
flat_meals = [id for id_list in all_meals for id in id_list]
unique_meals = set(flat_meals) - set(meal_dict['multiple']['id_list'])
len(unique_meals)

# CONCLUSION: Based on this alone, there is not enough data to continue analysis with Spoonacular.








# Get a list of columns of type int64 and float64, excluding 'id' and 'contributor_id'
calc_cols = simplified_df.select_dtypes(include=['int64', 'float64']).drop(columns=['ID'])

# Calculate IQR and bounds, then display the descriptive statistics
calc_stats = calc_cols.describe().T.drop(columns=['count'])
print(f'Columns with values above "max":{len(add_iqr(calc_stats))}\n{add_iqr(calc_stats)}')
calc_stats = calc_stats.T
calc_stats





# Create a histogram of the Likes column
simplified_df['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()


# Identify recipes with '0' likes
zero_likes = simplified_df.loc[simplified_df['Likes'] == 0].copy()
print(f'zero_likes: {zero_likes.shape}')





# Get the upper bounds for Likes
upper_likes = calc_stats.loc['upper_bounds', 'Likes']
likes_outliers = simplified_df.loc[simplified_df['Likes'] > upper_likes].copy()

# Display the count of outlier recipes
print(f"Number of outliers: {likes_outliers['Likes'].count()}")

# Create a histogram to understand distribution
likes_outliers['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes - Likes Outliers")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()


# Get the descriptive statistics for the outliers
outlier_stats = likes_outliers['Likes'].describe()
outlier_stats = add_iqr(outlier_stats)
print(outlier_stats)

# Get the DataFrame for outliers of outliers
outliers_squared = likes_outliers.loc[likes_outliers['Likes'] > outlier_stats['upper_bounds']]
print(f'outliers_squared: {outliers_squared.shape}')
outliers_squared.head()


# Plot the relationship between outliers of outliers' WW Smart Points vs Likes
x_values = outliers_squared['WW Smart Points']
y_values = outliers_squared['Calories (Amount)']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values, s=outliers_squared['Likes'], alpha=0.3)
linreg_plot(ax, x_values, y_values, 100, 100)
plt.show()


# Get Likes within the limits
likes_within = simplified_df.loc[simplified_df['Likes'] < upper_likes]

# Print the count of recipes within limits
print(f"Number of recipes within limits: {likes_within['Likes'].count()}")

# Create a histogram to understand distribution
likes_within['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes - Likes within Limits")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()








# Food.com columns
clean_recipes.columns


# Current Spoonacular data columns
simplified_df.columns





x_values = simplified_df['WW Smart Points']
y_values = simplified_df['Likes']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)








import numpy as np
points_bins = np.arange(-50, 105, 10)
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)


plt.plot(x_val, y_val)


binned_df['wws_points'].describe()


# Check recipes <40 wws_points
low_points = clean_recipes.loc[clean_recipes['wws_points'] < -40]
low_points


no_oamc = parse_tags(['oamc-freezer-make-ahead'], 'tags', clean_recipes, 'id')
test_df = clean_recipes.loc[~clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test_df

points_bins = np.arange(-40, 105, 10)
print(len(points_bins))
bins_df = pd.cut(test_df['wws_points'], bins=points_bins)
points_df = test_df.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)

test1_df = clean_recipes.loc[clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test1_df.describe()


# STATISTICAL TESTING and PLOTS
sample_set = clean_recipes['wws_points'].sample(n=500)
sample_set.hist(bins=15)

stats.shapiro(sample_set)


result = stats.anderson(clean_recipes['wws_points'])

print('Statistic: %.3f' % result.statistic)
p = 0
# interpret results
for i in range(len(result.critical_values)):
    slevel, cvalues = result.significance_level[i], result.critical_values[i]
    if result.statistic < result.critical_values[i]:
        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (slevel, cvalues))
    else:
        print('%.3f: %.3f, data does not look normal (reject H0)' % (slevel, cvalues))


# Parse the ingredients column
# Identify how to isolate each tag
tag_string = clean_recipes['ingredients'][1].strip("[]")
tag_string = tag_string.split(', ')
tag_string[62].strip("'")

# Get a list of unique tags
unique_ingredients = []
for string in clean_recipes['ingredients']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_ingredients:
            unique_ingredients.append(tag)

# Display the tags alphabetically
print(f"Number of unique ingredients: {len(unique_ingredients)}")

sorted_ingredients = sorted(unique_ingredients)
sorted_ingredients

for row in range(0, 52):
    sorted_ingredients[row] = sorted_ingredients[row].strip('"')
sorted_ingredients

# open file
with open('ingredients_commas.txt', 'w+') as f:
     
    # write elements of list
    for items in sorted_ingredients:
        f.write('%s, ' %items)
    print("File written successfully")
 
 
# close the file
f.close()
# # for line in sorted_ingredients[0:52]:
# #     print(line.strip('"'))
# sorted = [line.strip('"') for line in unique_ingredients[0:52]]
# unique_ingredients[0:52]


import food
report = food.get_report()
report[32]['Description']


categories = []
for row in range(len(report)):
    categories.append(report[row]['Category'])

new_data = set(categories)
old_data = set(sorted_ingredients)

# if (new_data & old_data):
#     print(new_data & old_data)
if len(new_data.intersection(old_data)) > 0:
    print(new_data.intersection(old_data))
else:
    print("no common elements")
new_data = set([word.lower() for word in list(new_data)])
old_data = set([word.lower() for word in list(old_data)])

if len(new_data.intersection(old_data)) > 0:
    intersect = new_data.intersection(old_data)
    print(len(intersect))
    print(intersect)
else:
    print("no common elements")

# Return dataframe with these common ingredients



# Recipes with negative points are high in protein
negative_points = clean_recipes.loc[clean_recipes['wws_points'] < 0].copy()
print(f'negative_points: {negative_points.shape}')
negative_points.head()


x_values = negative_points['wws_points']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['total_fat_g']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['wws_points']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


recipe_sample = clean_recipes.sample(n=500)
x_values = recipe_sample['wws_points']
y_values = recipe_sample['rating']

fig_test, ax_test = plt.subplots()
ax_test.scatter(x_values, y_values)
linreg_plot(ax_test, x_values, y_values, 80, 80)


# CREATE A BUBBLE PLOT? Bin the WW Smart Points, use the count of each bin as the bubble size (and plot this as the 3rd dimension)












