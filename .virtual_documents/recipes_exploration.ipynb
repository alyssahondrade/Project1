





# Import libraries
import os
import matplotlib.pyplot as plt
import pandas as pd
import requests
import scipy.stats as stats

# Import API keys
from alyssa_config import spoonacular_key, rapidapi_key
# from lakna_config import spoonacular_key


# Check number of items in the Resources subdirectories
# Purpose: Automate file naming
def recipe_folder():
    count = len(os.listdir('Resources/01_recipe_IDs'))-1
    return(count)

def raw_folder():
    count = len(os.listdir('Resources/02_raw_data'))-1
    return(count)

def simple_folder():
    count = len(os.listdir('Resources/03_simplified_data'))-1
    return(count)

def complex_folder():
    count = len(os.listdir('Resources/04_complex_test'))-1
    return(count)


# Purpose: Check each value in a list whether it exists in the tags
def tag_check(input_list):
    yes_tag = []
    not_tag = []
    alt_tag = []

    # Loop through each tag in the list
    for tag in input_list:
        if tag in unique_tags:
            yes_tag.append(tag)
        else:
            not_tag.append(tag)
            # Check for alternative tags
            for values in unique_tags:
                if tag in values:
                    alt_tag.append(values)
    return([yes_tag, not_tag, alt_tag])


# Purpose: Get a count of a given list from the tags column, and return its id
def parse_tags(input_list, df, save_col):
    # Create a dictionary to hold the count and list of IDs
    word_dict = {}
    for word in input_list:
        word_dict[word] = dict(count = 0, id_list = [])

    # Add a `multiple` key to track mutual exclusivity
    word_dict['multiple'] = dict(count = 0, id_list = [])

    for df_idx, string in df.iterrows():
        tag_list = string['tags'].strip("[]").split(', ')
    
        # Track mutual exclusivity for the meal types
        score = 0
        for word_idx in range(len(tag_list)):
            # Check the tag
            tag = tag_list[word_idx].strip("'")
            if tag in input_list:
                word_dict[tag]['count'] += 1
                word_dict[tag]['id_list'].append(string[save_col])
                score += 1
                # If >1 tag in a list, add to 'multiple' and continue
                if (score > 1):
                    word_dict['multiple']['count'] += 1
                    word_dict['multiple']['id_list'].append(string[save_col])
                    continue
    return(word_dict)


# Define a function to create linear regression plots
def linreg_plot(ax_object, x_values, y_values, x_coord, y_coord):
    # Calculate the lienar regression for x- and y-values
    (slope, intercept, rvalue, pvalue, stderr) = stats.linregress(x_values, y_values)

    # Get regression values and equation
    regression_values = x_values * slope + intercept
    line_equation = f'y = {round(slope, 2)}x + {round(intercept, 2)}'

    # Calculate the r-value
    print(f'The r-value is: {rvalue**2}')

    # Plot the linear regression
    ax_object.plot(x_values, regression_values, color='red')
    ax_object.annotate(
        line_equation, xy=(x_coord, y_coord), xycoords='figure points',
        fontsize = 15, color='red', weight='bold')
    plt.show()


# Parse 'nutrition' column and return nutritional values
def get_nutrition(string, nutritional_val):
    result = []
    # Based on longest name: Mono Unsaturated Fat
    longest_length = 90
    
    # Get the index of where the substring starts
    start_index = string.index(f"'{nutritional_val}")

    # Get the entire section of the nutritional value details
    section = string[start_index : start_index + longest_length]

    # Find the first closing curly bracket, set as the end_index
    curly_bracket = section.find("}") # returns -1 if not found
    
    if (curly_bracket > 0):
        end_index = start_index + curly_bracket
        section = string[start_index : end_index]

    section_list = section.split(",")
    for idx, part in enumerate(section_list):
        parsed_list = part.split(":")
        # Get the amount
        if (idx == 1):
            result.append(float(parsed_list[1].strip(" '")))
        if (idx == 2):
            result.append((parsed_list[1].strip(" '")))
        if (idx == 3):
            result.append(float(parsed_list[1].strip(" '")))
    return(result)





# Import the FOOD.COM datasets as DataFrames.
food_df = pd.read_csv('Resources/RAW_recipes.csv')
interactions_df = pd.read_csv('Resources/RAW_interactions.csv')





# Display the DataFrame
food_df.head()


# Display the DataFrame
interactions_df.head()


# Get the DataFrame dimensions
interactions_shape = interactions_df.shape
food_shape = food_df.shape

# Print findings
hash = f'{8*"#"}'
print(f'{hash} Shape {hash}')
print(f"food_df: {food_shape}")
print(f"interactions_df: {interactions_shape}")

# Get the columns
print(f'\n{hash} Columns {hash}')
print(f'food_df: {food_df.columns}')
print(f'interactions_df: {interactions_df.columns}')

# Get the datatypes
print(f'\n{hash} Data Types {hash}')
print(f'food_df: {food_df.dtypes}')
print(f'\ninteractions_df: {interactions_df.dtypes}')





# Check how many recipes have ratings
unique_ratings = len(interactions_df['recipe_id'].unique())

# Check how many recipes have a '0' rating
zero_rating_df = interactions_df.loc[interactions_df['rating'] == 0]['recipe_id'].unique()

# Drop the rows with a '0' rating
nonzero_df = interactions_df.loc[interactions_df['rating'] != 0]
nonzero_shape = nonzero_df.shape

# Print findings
print(f'Recipes with ratings: {unique_ratings} out of {food_shape[0]}')
print(f'Recipes with a "0" rating: {zero_rating_df.shape[0]}')
print(f'Updated shape: {nonzero_shape}')


# Create a DataFrame with the average ratings per recipe ID
food_ratings = nonzero_df.groupby('recipe_id')['rating'].mean().reset_index()

# Display the DataFrame
food_ratings.head()


# Rename the recipe ID column for merging with food_df
food_ratings = food_ratings.rename(columns={'recipe_id': 'id'})

# Merge the datasets and display updated DataFrame
merged_food = pd.merge(food_df, food_ratings, on='id')

# Confirm the row dimensions, to ensure correct merge
merged_shape = merged_food.shape
print(f'food_ratings rows: {food_ratings.shape[0]}')
print(f'merged_food rows: {merged_shape[0]}')

# Display the DataFrame
merged_food.head()


# Create a histogram of the ratings
merged_food['rating'].hist(bins=20)
plt.title("Food.com Recipes")
plt.xlabel("Rating")
plt.ylabel("Number of Recipes")
plt.show()





# Determine whether there are duplicate recipes by ID
dup_id = len(merged_food['id'].unique())
print(f'Unique recipe IDs: {dup_id} of {merged_shape[0]}')

# Determine whether there are duplicate recipes by name
dup_name = len(merged_food['name'].unique())
print(f'Unique recipe names: {dup_name} of {merged_shape[0]}')

# Get the duplicate names - remove to simplify the dataset
duplicate_names = merged_food.loc[merged_food.duplicated(['name'])]
dupname_shape = duplicate_names.shape
print(f'Number of duplicate rows to remove: {dupname_shape[0]}')

# Create a new DataFrame without the duplicates
updated_food = merged_food.loc[~merged_food.duplicated(['name'])].copy()
updated_shape = updated_food.shape
print(f'updated_food rows: {updated_shape[0]}')


# Display the DataFrame
updated_food.head()





# Identify how to isolate each tag
tag_string = updated_food['tags'][0].strip("[]")
tag_string = tag_string.split(', ')
tag_string[0].strip("'")

# Get a list of unique tags
unique_tags = []
for string in updated_food['tags']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_tags:
            unique_tags.append(tag)

# Display the tags alphabetically
print(f"Number of unique tags: {len(unique_tags)}")
sorted(unique_tags)





# Check the tags for meal types
meal_types = ["breakfast", "lunch", "dinner"]
match_meals = tag_check(meal_types)

# Print the results
print(f'Tags: {match_meals[0]}')
print(f'Not tags: {match_meals[1]}')
print(f'Alternative tags: {match_meals[2]}')

# Update the meal type list
meal_types[2] = "dinner-party"
print(f'Updated meal_types: {meal_types}')


# Get a count of each meal type
meal_dict = parse_tags(meal_types, updated_food, 'id')

# Get the list of all recipes with one meal type
combined_meals = []
for key in meal_dict:
    print(f"{key}: {meal_dict[key]['count']}")
    if (key != 'multiple'):
        combined_meals.append(meal_dict[key]['id_list'])

# Flatten combined_meals
flat_meals = [index for meal_list in combined_meals for index in meal_list]

# Remove multiples to get a list of all recipes with one meal type
unique_meals = [meal for meal in flat_meals if meal not in meal_dict['multiple']['id_list']]

# Print results
print(f'Total recipes with one meal type: {len(unique_meals)}')


# Reduce the dataset to contain recipes with only one meal type
reduced_food = updated_food.loc[updated_food['id'].isin(unique_meals)].copy()
reduced_shape = reduced_food.shape

print(f'reduced_food: {reduced_shape}')
reduced_food.head()





# Convert Spoonacular supported cuisines to a list
input_string = input("List to pass: ")

spoonacular_cuisines = input_string.split(' ')
spoonacular_cuisines = [word.lower() for word in spoonacular_cuisines]
spoonacular_cuisines # DOES NOT ACCOUNT FOR DOUBLE WORD.

# List comprehension to remove double words
double_words = ['eastern', 'european', 'latin', 'american', 'middle']
[spoonacular_cuisines.remove(word) for word in double_words]

# Return the two-word cuisines
spoonacular_cuisines += ['eastern european', 'latin american', 'middle eastern']
spoonacular_cuisines = sorted(spoonacular_cuisines)


# Check the tags for spoonacular cuisines
cuisine_match = tag_check(spoonacular_cuisines)[0]
print(f'Spoonacular Cuisines: {len(spoonacular_cuisines)}, {spoonacular_cuisines}\n')
print(f'Matched cuisines: {len(cuisine_match)}, {cuisine_match}')


# Get a count of each cuisine
cuisine_dict = parse_tags(cuisine_match, reduced_food, 'id')

# Get the list of all recipes with one meal type
combined_cuisines = []
for key in cuisine_dict:
    print(f"{key}: {cuisine_dict[key]['count']}")
    if (key != 'multiple'):
        combined_cuisines.append(cuisine_dict[key]['id_list'])

# Flatten combined_meals to get a list of unique recipes with one meal type
flat_cuisine = list(set([index for cuisine_list in combined_cuisines for index in cuisine_list]))

# Remove multiples to get a list of all recipes with one cuisine type
unique_cuisine = [cuisine for cuisine in flat_cuisine if cuisine not in cuisine_dict['multiple']['id_list']]
print(f'Total recipes with one cuisine type: {len(unique_cuisine)}')


# Reduce the dataset to contain recipes with only one cuisine
one_cuisine = reduced_food.loc[reduced_food['id'].isin(unique_cuisine)].copy()
print(f'reduced_food: {one_cuisine.shape}')
one_cuisine.head()





# Identify how to split the nutrition string and convert to float
test_string = one_cuisine['nutrition']

# Remove the square brackets
test_string = test_string[1].strip("[]")

# Split the string to a list
test_string = test_string.split(", ")

# Cast values to float
test_string = [float(value) for value in test_string]
test_string


# Identify how to convert PDV to nutrient quantity
conversion_ref = [65, 50, 2.4, 50, 20, 300]

# Remove 'Calories' from the test string
test_pdv = test_string[1:]

# Convert from PDV to absolute values
abs_values = []
for ref in range(len(conversion_ref)):
    abs_values.append(test_pdv[ref] * conversion_ref[ref] / 100)
abs_values


one_cuisine.head()


# Parse each value in the `nutrition` column
for df_idx, row in one_cuisine.iterrows():
    # Strip and split the string to a list
    values_list = row['nutrition'].strip("[]").split(", ")

    # Allocate each nutritional value to the correct column
    for idx, value in enumerate(values_list):
        if (idx == 0):
            one_cuisine.loc[df_idx, 'Calories'] = float(value)
        elif (idx == 1):
            one_cuisine.loc[df_idx, 'Total Fat (PDV)'] = float(value)
        elif (idx == 2):
            one_cuisine.loc[df_idx, 'Sugar (PDV)'] = float(value)
        elif (idx == 3):
            one_cuisine.loc[df_idx, 'Sodium (PDV)'] = float(value)
        elif (idx == 4):
            one_cuisine.loc[df_idx, 'Protein (PDV)'] = float(value)
        elif (idx == 5):
            one_cuisine.loc[df_idx, 'Saturated Fat (PDV)'] = float(value)
        elif (idx == 6):
            one_cuisine.loc[df_idx, 'Carbohydrates (PDV)'] = float(value)

# Display the DataFrame
one_cuisine.head()


# Convert PDV nutritional values to absolute values (in grams)
merged_subset = one_cuisine[[
    'Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

for df_idx, row in merged_subset.iterrows():
    for col_idx in range(len(row)):
        result = row[col_idx] * conversion_ref[col_idx] / 100
        if (col_idx == 0):
            one_cuisine.loc[df_idx, 'total_fat_g'] = result
        elif (col_idx == 1):
            one_cuisine.loc[df_idx, 'sugar_g'] = result
        elif (col_idx == 2):
            one_cuisine.loc[df_idx, 'sodium_g'] = result
        elif (col_idx == 3):
            one_cuisine.loc[df_idx, 'protein_g'] = result
        elif (col_idx == 4):
            one_cuisine.loc[df_idx, 'sat_fat_g'] = result
        elif (col_idx == 5):
            one_cuisine.loc[df_idx, 'carbs_g'] = result

# Display the DataFrame
print(f'one_cuisine: {one_cuisine.shape}')
one_cuisine.head()





for df_idx, row in one_cuisine.iterrows():
    calories = row.Calories
    sat_fat = row.sat_fat_g
    sugar = row.sugar_g
    protein = row.protein_g
    one_cuisine.loc[df_idx, 'wws_points'] = round((calories * 0.0305) + (sat_fat * 0.275) + (sugar * 1.2) - (protein * 0.98), 0)

# Convert wws_points to integers and display the results
one_cuisine = one_cuisine.astype({'wws_points':'int64'})
one_cuisine.head()








# Check DataFrame datatypes
one_cuisine.dtypes


# Get a list of columns of type int64 and float64, excluding 'id' and 'contributor_id'
calc_cols = one_cuisine.select_dtypes(include=['int64', 'float64']).drop(columns=['id', 'contributor_id'])

# Display the descriptive statistics
calc_stats = calc_cols.describe().T.drop(columns=['count'])
calc_stats


# Calculate the IQR and identify outliers
iqr_list = []
lowerbounds_list = []
upperbounds_list = []
above_max = []
for df_idx, row in calc_stats.iterrows():
    lower = calc_stats.loc[df_idx, '25%']
    upper = calc_stats.loc[df_idx, '75%']
    iqr = upper - lower
    
    iqr_list.append(iqr)
    lowerbounds_list.append(lower - 1.5*iqr)
    upperbounds_list.append(upper + 1.5*iqr)

    if (calc_stats.loc[df_idx, 'max'] > upper):
        above_max.append(df_idx)

# Add results to the descriptive statistics table
calc_stats['iqr'] = iqr_list
calc_stats['lower_bounds'] = lowerbounds_list
calc_stats['upper_bounds'] = upperbounds_list

# Display the updated DataFrame
print(f'Columns with values above "max":\n{above_max}')
calc_stats = calc_stats.T
calc_stats





# Get upper_bounds for PDV columns
upper_pdv = calc_stats.loc['upper_bounds', ['Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

# Create a dictionary to hold the count and IDs
pdv_dict = {}
for col in upper_pdv.index:
    pdv_dict[col] = dict(count = 0, id_list = [])

# Loop through upper_pdv to get the count and IDs
for idx, value in enumerate(upper_pdv):
    col_name = upper_pdv.index[idx]
    pdv_dict[col_name]['count'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id'].count()
    pdv_dict[col_name]['id_list'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id']

# Get the list of all recipes which exceeds upper bounds for PDV
combined_pdv = []
for key in pdv_dict:
    print(f"{key}: {pdv_dict[key]['count']}")
    combined_pdv.append(pdv_dict[key]['id_list'])

# Flatten combined_pdv
flat_pdv = list(set([index for pdv_list in combined_pdv for index in pdv_list]))
print(f'Total recipes which exceed upper bounds for PDV: {len(flat_pdv)}')


# Remove outliers from PDV columns: These are likely as a result of no information on serving sizes
within_pdv = one_cuisine.loc[~one_cuisine['id'].isin(flat_pdv)].copy()
print(f'within_pdv: {within_pdv.shape}')
within_pdv.head()





# Identify recipes with '0' minutes cooking time
zeromin_recipes = within_pdv.loc[within_pdv['minutes'] == 0]
print(f'zeromin_recipes: {zeromin_recipes.shape}')
zeromin_recipes.sort_values(by='minutes', ascending=False)


# Get the upper bounds value from the 'minutes' column
minutes_upper = calc_stats.loc['upper_bounds', 'minutes']

# Identify recipes with very long cooking times
long_recipes = within_pdv.loc[within_pdv['minutes'] > minutes_upper]
print(f'long_recipes: {long_recipes.shape}')
long_recipes.sort_values(by='minutes', ascending=False)





# How many of the long recipes had PDV outliers?
pdv_long = one_cuisine.loc[one_cuisine['id'].isin(flat_pdv)]
pdv_long.sort_values(by='minutes', ascending=False)


pdv_mins = pdv_long.loc[pdv_long['minutes'] > minutes_upper]
print(f'Recipes with PDV outliers: {pdv_long.shape[0]}')
print(f'Long recipes with PDV outliers: {pdv_mins.shape[0]}')


# How many of the long recipes are highly rated?
best_long = long_recipes.loc[long_recipes['rating'] == 5].copy()
best_long['minutes'].describe()





# Remove recipes greater than the calculated upper bounds AND recipes with '0' minutes cooking time
clean_recipes = within_pdv.loc[(within_pdv['minutes'] < minutes_upper) & (within_pdv['minutes'] > 0)].copy()
print(f'clean_recipes: {clean_recipes.shape}')
clean_recipes.head()





# Get the descriptive statistics summary
clean_recipes['wws_points'].describe()


# Create a histogram of the points
clean_recipes['wws_points'].hist(bins=20)
plt.title("Food.com Recipes")
plt.xlabel("WW Smart Points")
plt.ylabel("Number of Recipes")
plt.show()


# Check for Gaussian distribution - using Shapiro-Wilk test
# Note: Shapiro-Wilk is sensitive to sample size
sample_set = clean_recipes['wws_points'].sample(n=500)
sample_set.hist(bins=15)
plt.title("Food.com Recipes")
plt.xlabel("Weight Watchers (WW) Smart Points")
plt.ylabel("Number of Recipes")

stats.shapiro(sample_set)
# A very high W-statistic suggests a good fit to a normal distribution.
# A near-zero p-value, for a Shapiro-Wilk test, rejects the null hypothesis, meaning NOT a normal distribution.








# API REQUEST - Complex Search.
def complex_search():
    recipe_url = "https://api.spoonacular.com/recipes/complexSearch"
    
    # Set request parameters
    recipe_params = {
        'apiKey': spoonacular_key,
        'number': 100}
    
    # Request a list of recipes.
    recipe_response = requests.get(recipe_url, recipe_params).json()
    
    # Output response to a csv
    recipes_df = pd.DataFrame(recipe_response['results'])
    recipes_df.to_csv(f'Resources/01_recipe_IDs/initial_recipes_0.csv', index=False)

# Uncomment the line below to run the API request
# complex_search()


# Import and display complex search results
recipes_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_0.csv')
print(f'recipes_df: {recipes_df.shape}')
recipes_df.head()


# API REQUEST - Recipe Information
def recipe_info(output_name):
    recipe_info = []
    for id in recipes_df['id']:
        info_url = f'https://api.spoonacular.com/recipes/{id}/information?'
        info_params = {
            'apiKey': spoonacular_key,
            'includeNutrition': "true"}
    
        info_response = requests.get(info_url, info_params).json()
        recipe_info.append(info_response)
    
    info_df = pd.DataFrame(recipe_info)
    info_df.to_csv(output_name, index=False)

# Uncomment the line below to run the API request
# recipe_info('Resources/02_raw_data/info_master_0.csv')


# While the API response is accessible, identify attributes of interest
def check_response():
    recipe_info[0].keys()
    
    # Generic metadata
    recipe_info[0]['id']
    recipe_info[0]['title']
    recipe_info[0]['aggregateLikes'] # will need to convert this to a 5-point scale
    recipe_info[0]['extendedIngredients'] # can use len() to get the number of ingredients
    recipe_info[0]['weightWatcherSmartPoints'] # research the equation for this
    recipe_info[0]['cuisines'] # identify recipes with non-empty results
    recipe_info[0]['dishTypes'] # will need to select a simpler subset
    
    # Nutritional values - amount, unit, percent of daily needs
    recipe_info[0]['nutrition']['nutrients'][0] # Calories
    recipe_info[0]['nutrition']['nutrients'][2] # Saturated Fat
    recipe_info[0]['nutrition']['nutrients'][5] # Sugar
    recipe_info[0]['nutrition']['nutrients'][7] # Sodium
    recipe_info[0]['nutrition']['nutrients'][8] # Protein

# Uncomment the line below to run the function after an API request
# check_response()


# Import and display recipe information results
info_df = pd.read_csv(f'Resources/02_raw_data/info_master_0.csv')

# Display the DataFrame and its columns
print(info_df.columns)
info_df.head()


# Parse the API response - all relevant columns, and account for different units
def parse_response(output_name):
    # Metadata
    recipe_id = []
    recipe_name = []
    recipe_likes = []
    meal_type = []
    cuisines = []
    num_ingredients = []
    ww_points = []
    
    # Calories
    calories_amt = []
    calories_unit = []
    calories_pct = []
    
    # Saturated Fat
    satfat_amt = []
    satfat_unit = []
    satfat_pct = []
    
    # Sugar
    sugar_amt = []
    sugar_unit = []
    sugar_pct = []
    
    # Sodium
    sodium_amt = []
    sodium_unit = []
    sodium_pct = []
    
    # Protein
    protein_amt = []
    protein_unit = []
    protein_pct = []
    
    for idx, row in enumerate(recipe_info):
        try:
            recipe_id.append(recipe_info[idx]['id'])
            recipe_name.append(recipe_info[idx]['title'])
            recipe_likes.append(recipe_info[idx]['aggregateLikes'])
            meal_type.append(recipe_info[idx]['dishTypes'])
            cuisines.append(recipe_info[idx]['cuisines'])
            num_ingredients.append(len(recipe_info[idx]['extendedIngredients']))
            ww_points.append(recipe_info[idx]['weightWatcherSmartPoints'])
            
            calories_amt.append(recipe_info[idx]['nutrition']['nutrients'][0]['amount'])
            calories_unit.append(recipe_info[idx]['nutrition']['nutrients'][0]['unit'])
            calories_pct.append(recipe_info[idx]['nutrition']['nutrients'][0]['percentOfDailyNeeds'])
            
            satfat_amt.append(recipe_info[idx]['nutrition']['nutrients'][2]['amount'])
            satfat_unit.append(recipe_info[idx]['nutrition']['nutrients'][2]['unit'])
            satfat_pct.append(recipe_info[idx]['nutrition']['nutrients'][2]['percentOfDailyNeeds'])
            
            sugar_amt.append(recipe_info[idx]['nutrition']['nutrients'][5]['amount'])
            sugar_unit.append(recipe_info[idx]['nutrition']['nutrients'][5]['unit'])
            sugar_pct.append(recipe_info[idx]['nutrition']['nutrients'][5]['percentOfDailyNeeds'])
            
            sodium_amt.append(recipe_info[idx]['nutrition']['nutrients'][7]['amount'])
            sodium_unit.append(recipe_info[idx]['nutrition']['nutrients'][7]['unit'])
            sodium_pct.append(recipe_info[idx]['nutrition']['nutrients'][7]['percentOfDailyNeeds'])
            
            protein_amt.append(recipe_info[idx]['nutrition']['nutrients'][8]['amount'])
            protein_unit.append(recipe_info[idx]['nutrition']['nutrients'][8]['unit'])
            protein_pct.append(recipe_info[idx]['nutrition']['nutrients'][8]['percentOfDailyNeeds'])
        
        except:
            print(idx)
    
    spoonacular_df = pd.DataFrame({
        'ID': recipe_id,
        'Name': recipe_name,
        'Likes': recipe_likes,
        'Meal Type': meal_type,
        'Cuisines': cuisines,
        'N_ingredients': num_ingredients,
        'WW Smart Points': ww_points,
        'Calories (Amount)': calories_amt,
        'Calories (Unit)': calories_unit,
        'Calories (% of Daily Needs)': calories_pct,
        'Saturated Fat (Amount)': satfat_amt,
        'Saturated Fat (Unit)': satfat_unit,
        'Saturated Fat (% of Daily Needs)': satfat_pct,
        'Sugar (Amount)': sugar_amt,
        'Sugar (Unit)': sugar_unit,
        'Sugar (% of Daily Needs)': sugar_pct,
        'Sodium (Amount)': sodium_amt,
        'Sodium (Unit)': sodium_unit,
        'Sodium (% of Daily Needs)': sodium_pct,
        'Protein (Amount)': protein_amt,
        'Protein (Unit)': protein_unit,
        'Protein (% of Daily Needs)': protein_pct})
    
    spoonacular_df.to_csv(output_name, index=False)

# Uncomment the line below to run the function after an API request
# parse_complex('Resources/03_simplified_data/initial_nutrition_0.csv')


# Import and display recipe information results
simple_df = pd.read_csv(f'Resources/03_simplified_data/initial_nutrition_0.csv')

# Display the DataFrame and its columns
print(simple_df.columns)
simple_df.head()


# Check the units columns
units_columns = ['Calories (Unit)', 'Saturated Fat (Unit)', 'Sugar (Unit)', 'Sodium (Unit)', 'Protein (Unit)']
for col in units_columns:
    check = simple_df['Protein (Unit)'].unique()
    print(f"Unit check: {col} {check}")





# API REQUEST - Random Recipe
def random_recipe(output_name):
    random_url = "https://api.spoonacular.com/recipes/random?"
    
    # Set request parameters
    random_params = {
        'apiKey': spoonacular_key,
        'number': 100}
    
    # Request a list of random recipe IDs.
    random_response = requests.get(random_url, random_params).json()
    
    # Output response to a csv
    random_df = pd.DataFrame(random_response['recipes'])
    random_df.to_csv(output_name, index=False)

# Uncomment the line below to run the API request
# random_recipe('Resources/01_recipe_IDs/initial_recipes_4.csv')


# Import and display random recipes results
random_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_4.csv')
random_df.head()


# API REQUEST - Recipe Information and parse response

# Uncomment the line below to run the API request
# recipe_info('Resources/02_raw_data/info_master_4.csv')

# Uncomment the line below to parse the API request response
# parse_response('Resources/03_simplified_data/initial_nutrition_4.csv')





# API REQUEST - Random Recipe and Nutrition by ID

# Uncomment the line below to run the API request
# recipe_info('Resources/02_raw_data/info_master_5.csv')

def nutrition_id(output_name):
    nutrition_info = []
    for id in random_df['id']:
        nutrition_url = f"https://api.spoonacular.com/recipes/{id}/nutritionWidget.json"
        nutrition_params = {'apiKey': spoonacular_key}

        # Request recipe information for each recipe ID
        nutrition_response = requests.get(nutrition_url, nutrition_params).json()
        nutrition_info.append(nutrition_response)

    # Output response to a csv
    nutrition_df = pd.DataFrame(nutrition_info)
    nutrition_df.to_csv()

# Uncomment the line below to run the API request
# nutrition_id('Resources/02_raw_data/info_master_5.csv')


# Parse the response - added carbohydrates
def parse_metadata(input_csv, output_name):
    # Metadata
    metadata = pd.read_csv(input_csv)
    num_ingredients = []
    carbs = []

    # Calories
    calories_amt = []
    calories_unit = []
    calories_pct = []
    
    # Saturated Fat
    satfat_amt = []
    satfat_unit = []
    satfat_pct = []
    
    # Sugar
    sugar_amt = []
    sugar_unit = []
    sugar_pct = []
    
    # Sodium
    sodium_amt = []
    sodium_unit = []
    sodium_pct = []
    
    # Protein
    protein_amt = []
    protein_unit = []
    protein_pct = []
    
    for idx, row in enumerate(nutrition_info):
        try:
            num_ingredients.append(len(nutrition_info[idx]['ingredients']))
            carbs.append(nutrition_info[idx]['carbs'])
            calories_amt.append(nutrition_info[idx]['nutrients'][0]['amount'])
            calories_unit.append(nutrition_info[idx]['nutrients'][0]['unit'])
            calories_pct.append(nutrition_info[idx]['nutrients'][0]['percentOfDailyNeeds'])
            
            satfat_amt.append(nutrition_info[idx]['nutrients'][2]['amount'])
            satfat_unit.append(nutrition_info[idx]['nutrients'][2]['unit'])
            satfat_pct.append(nutrition_info[idx]['nutrients'][2]['percentOfDailyNeeds'])
            
            sugar_amt.append(nutrition_info[idx]['nutrients'][5]['amount'])
            sugar_unit.append(nutrition_info[idx]['nutrients'][5]['unit'])
            sugar_pct.append(nutrition_info[idx]['nutrients'][5]['percentOfDailyNeeds'])
            
            sodium_amt.append(nutrition_info[idx]['nutrients'][7]['amount'])
            sodium_unit.append(nutrition_info[idx]['nutrients'][7]['unit'])
            sodium_pct.append(nutrition_info[idx]['nutrients'][7]['percentOfDailyNeeds'])
            
            protein_amt.append(nutrition_info[idx]['nutrients'][8]['amount'])
            protein_unit.append(nutrition_info[idx]['nutrients'][8]['unit'])
            protein_pct.append(nutrition_info[idx]['nutrients'][8]['percentOfDailyNeeds'])
        
        except:
            print(idx)
    
    spoonacular_df = pd.DataFrame({
        'ID': metadata['id'],
        'Name': metadata['title'],
        'Likes': metadata['aggregateLikes'],
        'Meal Type': metadata['dishTypes'],
        'Cuisines': metadata['cuisines'],
        'N_ingredients': num_ingredients,
        'WW Smart Points': metadata['weightWatcherSmartPoints'],
        'Calories (Amount)': calories_amt,
        'Calories (Unit)': calories_unit,
        'Calories (% of Daily Needs)': calories_pct,
        'Saturated Fat (Amount)': satfat_amt,
        'Saturated Fat (Unit)': satfat_unit,
        'Saturated Fat (% of Daily Needs)': satfat_pct,
        'Sugar (Amount)': sugar_amt,
        'Sugar (Unit)': sugar_unit,
        'Sugar (% of Daily Needs)': sugar_pct,
        'Sodium (Amount)': sodium_amt,
        'Sodium (Unit)': sodium_unit,
        'Sodium (% of Daily Needs)': sodium_pct,
        'Protein (Amount)': protein_amt,
        'Protein (Unit)': protein_unit,
        'Protein (% of Daily Needs)': protein_pct,
        'Carbohydrates': carbs})
    
    spoonacular_df.to_csv(output_name, index=False)

# Uncomment the lines below to run the function after an API request
# parse_metadata(
#     'Resources/01_recipe_IDs/initial_recipes_5.csv',
#     'Resources/03_simplified_data/initial_nutrition_5.csv')





def spoonacular_v1():
    ### PHASE 1 - Get Recipe IDs ###
    # API REQUEST - Random Recipe
    random_url = "https://api.spoonacular.com/recipes/random?"

    # Set request parameters
    random_params = {
        'apiKey': spoonacular_key,
        'number': 100}
    
    # Request a list of random recipe IDs.
    random_response = requests.get(random_url, random_params).json()
    
    # Output response to a csv
    random_df = pd.DataFrame(random_response['recipes'])
    random_df.to_csv(f'Resources/01_recipe_IDs/initial_recipes_{recipe_folder()}.csv', index=False)

    ### PHASE 2 - Get Nutrition by ID ###
    # API REQUEST - Nutrition by ID
    nutrition_info = []
    for id in random_df['id']:
        nutrition_url = f'https://api.spoonacular.com/recipes/{id}/nutritionWidget.json'
        nutrition_params = {'apiKey': spoonacular_key}

        # Request recipe information for each recipe ID
        nutrition_response = requests.get(nutrition_url, nutrition_params).json()
        nutrition_info.append(nutrition_response)

    # Output response to a csv
    nutrition_df = pd.DataFrame(nutrition_info)
    nutrition_df.to_csv(f'Resources/02_raw_data/info_master_{raw_folder()}.csv')

    ### PHASE 3 - Parse response ###
    # Metadata
    metadata = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_{recipe_folder()-1}.csv')
    num_ingredients = []
    carbs = []

    # Calories
    calories_amt = []
    calories_unit = []
    calories_pct = []
    
    # Saturated Fat
    satfat_amt = []
    satfat_unit = []
    satfat_pct = []
    
    # Sugar
    sugar_amt = []
    sugar_unit = []
    sugar_pct = []
    
    # Sodium
    sodium_amt = []
    sodium_unit = []
    sodium_pct = []
    
    # Protein
    protein_amt = []
    protein_unit = []
    protein_pct = []
    
    for idx, row in enumerate(nutrition_info):
        try:
            num_ingredients.append(len(nutrition_info[idx]['ingredients']))
            carbs.append(nutrition_info[idx]['carbs'])
            calories_amt.append(nutrition_info[idx]['nutrients'][0]['amount'])
            calories_unit.append(nutrition_info[idx]['nutrients'][0]['unit'])
            calories_pct.append(nutrition_info[idx]['nutrients'][0]['percentOfDailyNeeds'])
            
            satfat_amt.append(nutrition_info[idx]['nutrients'][2]['amount'])
            satfat_unit.append(nutrition_info[idx]['nutrients'][2]['unit'])
            satfat_pct.append(nutrition_info[idx]['nutrients'][2]['percentOfDailyNeeds'])
            
            sugar_amt.append(nutrition_info[idx]['nutrients'][5]['amount'])
            sugar_unit.append(nutrition_info[idx]['nutrients'][5]['unit'])
            sugar_pct.append(nutrition_info[idx]['nutrients'][5]['percentOfDailyNeeds'])
            
            sodium_amt.append(nutrition_info[idx]['nutrients'][7]['amount'])
            sodium_unit.append(nutrition_info[idx]['nutrients'][7]['unit'])
            sodium_pct.append(nutrition_info[idx]['nutrients'][7]['percentOfDailyNeeds'])
            
            protein_amt.append(nutrition_info[idx]['nutrients'][8]['amount'])
            protein_unit.append(nutrition_info[idx]['nutrients'][8]['unit'])
            protein_pct.append(nutrition_info[idx]['nutrients'][8]['percentOfDailyNeeds'])
        
        except:
            print(idx)
    
    spoonacular_df = pd.DataFrame({
        'ID': metadata['id'],
        'Name': metadata['title'],
        'Likes': metadata['aggregateLikes'],
        'Meal Type': metadata['dishTypes'],
        'Cuisines': metadata['cuisines'],
        'N_ingredients': num_ingredients,
        'WW Smart Points': metadata['weightWatcherSmartPoints'],
        'Calories (Amount)': calories_amt,
        'Calories (Unit)': calories_unit,
        'Calories (% of Daily Needs)': calories_pct,
        'Saturated Fat (Amount)': satfat_amt,
        'Saturated Fat (Unit)': satfat_unit,
        'Saturated Fat (% of Daily Needs)': satfat_pct,
        'Sugar (Amount)': sugar_amt,
        'Sugar (Unit)': sugar_unit,
        'Sugar (% of Daily Needs)': sugar_pct,
        'Sodium (Amount)': sodium_amt,
        'Sodium (Unit)': sodium_unit,
        'Sodium (% of Daily Needs)': sodium_pct,
        'Protein (Amount)': protein_amt,
        'Protein (Unit)': protein_unit,
        'Protein (% of Daily Needs)': protein_pct,
        'Carbohydrates': carbs})
    
    spoonacular_df.to_csv(f'Resources/03_simplified_data/initial_nutrition_{simple_folder()}.csv', index=False)

# Uncomment the line below to run the API request
# spoonacular_v1()





def spoonacular_v2():
    ### PHASE 1 - Get Recipes with all information required ###
    # API REQUEST - Complex Search.
    recipe_url = "https://api.spoonacular.com/recipes/complexSearch"
    
    # Set request parameters
    recipe_params = {
        'apiKey': spoonacular_key,
        'number': 100,
        'cuisine': cuisine_match,
        'type': "breakfast", # Run again with "lunch" and "dinner"
        'addRecipeNutrition': True}
    
    # Request a list of recipes.
    recipe_response = requests.get(recipe_url, recipe_params).json()
    
    # Output response to a csv
    recipes_df = pd.DataFrame(recipe_response['results'])
    recipes_df.to_csv(f'Resources/03_raw_data/recipe_data_{raw_folder()}.csv', index=False)

    ### PHASE 2 - Parse response ###
    # Metadata
    recipe_id = []
    recipe_name = []
    recipe_likes = []
    meal_type = []
    cuisines = []
    num_ingredients = []
    ww_points = []
    mins_cook = []
    mins_prep = []
    
    # Calories
    calories_amt = []
    calories_unit = []
    calories_pct = []
    
    # Saturated Fat
    satfat_amt = []
    satfat_unit = []
    satfat_pct = []
    
    # Sugar
    sugar_amt = []
    sugar_unit = []
    sugar_pct = []
    
    # Sodium
    sodium_amt = []
    sodium_unit = []
    sodium_pct = []
    
    # Protein
    protein_amt = []
    protein_unit = []
    protein_pct = []
    
    # Carbohydrates
    carbs_amt = []
    carbs_unit = []
    carbs_pct = []
    
    recipe_info = recipe_response['results']
    for idx, row in enumerate(recipe_info):
        recipe_id.append(recipe_info[idx]['id'])
        recipe_name.append(recipe_info[idx]['title'])
        recipe_likes.append(recipe_info[idx]['aggregateLikes'])
        meal_type.append(recipe_info[idx]['dishTypes'])
        cuisines.append(recipe_info[idx]['cuisines'])
        num_ingredients.append(len(recipe_info[idx]['nutrition']['ingredients']))
        ww_points.append(recipe_info[idx]['weightWatcherSmartPoints'])
        mins_cook.append(recipe_info[idx]['cookingMinutes'])
        mins_prep.append(recipe_info[idx]['preparationMinutes'])
        
        calories_amt.append(recipe_info[idx]['nutrition']['nutrients'][0]['amount'])
        calories_unit.append(recipe_info[idx]['nutrition']['nutrients'][0]['unit'])
        calories_pct.append(recipe_info[idx]['nutrition']['nutrients'][0]['percentOfDailyNeeds'])
        
        satfat_amt.append(recipe_info[idx]['nutrition']['nutrients'][2]['amount'])
        satfat_unit.append(recipe_info[idx]['nutrition']['nutrients'][2]['unit'])
        satfat_pct.append(recipe_info[idx]['nutrition']['nutrients'][2]['percentOfDailyNeeds'])
        
        sugar_amt.append(recipe_info[idx]['nutrition']['nutrients'][5]['amount'])
        sugar_unit.append(recipe_info[idx]['nutrition']['nutrients'][5]['unit'])
        sugar_pct.append(recipe_info[idx]['nutrition']['nutrients'][5]['percentOfDailyNeeds'])
        
        sodium_amt.append(recipe_info[idx]['nutrition']['nutrients'][7]['amount'])
        sodium_unit.append(recipe_info[idx]['nutrition']['nutrients'][7]['unit'])
        sodium_pct.append(recipe_info[idx]['nutrition']['nutrients'][7]['percentOfDailyNeeds'])
        
        protein_amt.append(recipe_info[idx]['nutrition']['nutrients'][8]['amount'])
        protein_unit.append(recipe_info[idx]['nutrition']['nutrients'][8]['unit'])
        protein_pct.append(recipe_info[idx]['nutrition']['nutrients'][8]['percentOfDailyNeeds'])
    
        carbs_amt.append(recipe_info[idx]['nutrition']['nutrients'][8]['amount'])
        carbs_unit.append(recipe_info[idx]['nutrition']['nutrients'][8]['unit'])
        carbs_pct.append(recipe_info[idx]['nutrition']['nutrients'][8]['percentOfDailyNeeds'])
    
    spoonacular_df = pd.DataFrame({
        'ID': recipe_id,
        'Name': recipe_name,
        'Likes': recipe_likes,
        'Meal Type': meal_type,
        'Cuisines': cuisines,
        'N_ingredients': num_ingredients,
        'WW Smart Points': ww_points,
        'Cooking Minutes': mins_cook,
        'Prep Minutes': mins_prep,
        'Calories (Amount)': calories_amt,
        'Calories (Unit)': calories_unit,
        'Calories (% of Daily Needs)': calories_pct,
        'Saturated Fat (Amount)': satfat_amt,
        'Saturated Fat (Unit)': satfat_unit,
        'Saturated Fat (% of Daily Needs)': satfat_pct,
        'Sugar (Amount)': sugar_amt,
        'Sugar (Unit)': sugar_unit,
        'Sugar (% of Daily Needs)': sugar_pct,
        'Sodium (Amount)': sodium_amt,
        'Sodium (Unit)': sodium_unit,
        'Sodium (% of Daily Needs)': sodium_pct,
        'Protein (Amount)': protein_amt,
        'Protein (Unit)': protein_unit,
        'Protein (% of Daily Needs)': protein_pct,
        'Carbs (Amount)': carbs_amt,
        'Carbs (Unit)': carbs_unit,
        'Carbs (% of Daily Needs)': carbs_pct})
    
    spoonacular_df.to_csv(f'Resources/04_complex_test/simplified_data_{complex_folder()}.csv', index=False)

# Uncomment the line below to run the API request
# spoonacular_v2()





# Get list of subdirectories in the Resources folder
from pathlib import Path
subdir_list = []
for path in Path('Resources').iterdir():
    if (path == Path('Resources/.ipynb_checkpoints')):
        continue
    elif path.is_dir():
        subdir_list.append(path)

# Loop over folders without simplified data
raw_ids = []
raw_files = []
for dir in subdir_list[0:2]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints'):
            filename = f'{dir}/{file.name}'
            csv_df = pd.read_csv(filename)
            try:
                raw_ids.append(csv_df['id'])
                raw_files.append(filename)
            except:
                # If no 'id' column, move on to the next csv
                continue

# Get a unique list of recipe IDs
raw_list = [id for row in raw_ids for id in row]
unique_raw = list(set(raw_list))
print(f'Unique out of raw data: {len(unique_raw)} of {len(raw_list)}')

# Loop over folders with simplified data
simple_ids = []
simple_files = []
for dir in subdir_list[2:]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints') & (file.name.split('.')[-1] == 'csv'):
                filename = f'{dir}/{file.name}'
                csv_df = pd.read_csv(filename)
                try:
                    try:
                        simple_ids.append(csv_df['id'])
                    except:
                        simple_ids.append(csv_df['ID'])
                    simple_files.append(filename)
                except:
                    # If no 'id' column, move on to the next csv
                    continue

# Get a unique list of recipe IDs
simple_list = [id for row in simple_ids for id in row]
unique_simple = list(set(simple_list))
print(f'Unique out of simplified data: {len(unique_simple)} of {len(simple_list)}')

# Identify the intersection between the unique raw and unique simplified data
intersect_ids = []
missing_ids = set(unique_raw).difference(set(unique_simple))
if set(unique_raw) & set(unique_simple):
    intersect_ids.append(set(unique_raw) & set(unique_simple))
print(f'Total intersection: {len(intersect_ids[0])}')
print(f'Total missing: {len(missing_ids)}')


# Import one file from 03_simplified_data as a reference
simple_ref = pd.read_csv('Resources/03_simplified_data/initial_nutrition_7.csv').columns
simple_ref


# Import one file from 04_comple_test as a reference
complex_ref = pd.read_csv('Resources/04_complex_test/simplified_data_0.csv').columns
complex_ref


# Read each simplified data file and create a DataFrame
df_list = []
for file in simple_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
simplified_df = pd.concat(df_list, ignore_index=True)

# Identify duplicate IDs
duplicate_IDs = simplified_df.loc[simplified_df.duplicated(['ID'])]
print(f'Number of duplicate rows to remove: {duplicate_IDs.shape[0]}')

# Create a new DataFrame without the duplicates
simplified_df = simplified_df.loc[~simplified_df.duplicated(['ID'])].copy()

# Display the DataFrame, columns, and shape
print(f'simplified_df: {simplified_df.shape}')
print(simplified_df.columns)
simplified_df.head()


# Read each raw data file and create a DataFrame
df_list = []
for file in raw_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
raw_df = pd.concat(df_list, ignore_index=True)

# Drop rows with missing IDs
raw_df = raw_df.loc[~raw_df['id'].isna()]

# Reduce DataFrame to unique IDs
raw_df = raw_df.loc[~raw_df.duplicated(['id'])].copy()

# Display the DataFrame, columns, and shape
print(f'raw_df: {raw_df.shape}')
print(raw_df.columns)
raw_df.head()

#### NUMBERS DON'T MATCH ######
# raw_ids = raw_df['id'].unique()
# print(len(unique_raw))
# print(len(set(raw_ids)))

# diff = set(raw_ids).difference(set(unique_raw))
# len(diff)


# Minimum columns required
min_cols = ['ID', 'Name', 'Likes', 'Meal Type', 'Cuisines', 'N_ingredients',
       'WW Smart Points', 'Cooking Minutes', 'Prep Minutes',
       'Calories (Amount)', 'Calories (Unit)', 'Calories (% of Daily Needs)',
       'Saturated Fat (Amount)', 'Saturated Fat (Unit)',
       'Saturated Fat (% of Daily Needs)', 'Sugar (Amount)', 'Sugar (Unit)',
       'Sugar (% of Daily Needs)', 'Sodium (Amount)', 'Sodium (Unit)',
       'Sodium (% of Daily Needs)', 'Protein (Amount)', 'Protein (Unit)',
       'Protein (% of Daily Needs)', 'Carbs (Amount)', 'Carbs (Unit)',
       'Carbs (% of Daily Needs)']

# Identify columns with missing data
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    print(f'{col}: {num_na}')


# Get the list of recipe IDs with missing data
missing_recipes = check_na['ID']

# Check if the missing data have rows in raw_df
find_missing = raw_df.loc[raw_df['id'].isin(missing_recipes)]

# Check if there is a nutrition column for the missing data
find_nutrition = raw_df.loc[~raw_df['nutrition'].isna()]
print(f'Missing recipes in raw_df with a nutrition column: {len(find_nutrition)}')


# Test the get_nutrition() function
test_nutrition = get_nutrition(raw_df['nutrition'][0], 'Carbohydrates')
test_nutrition


# Populate simplified_df with the missing data
for df_idx, row in find_nutrition.iterrows():
    result = get_nutrition(row['nutrition'], 'Carbohydrates')
    recipe_id = find_nutrition.loc[df_idx, 'id']
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = result[0]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = result[1]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = result[2]

# Check columns with missing data
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    print(f'{col}: {num_na}')


# Identify the recipes with still missing values
still_missing = simplified_df.loc[simplified_df['ID'].isin(check_na['ID'])]
check_column = still_missing.loc[~still_missing['Carbohydrates'].isna()]
print(f'Carbohydrates column with values: {len(check_column)}')
still_missing.head() # Data is in 'Carbohydrates' column


for df_idx, row in still_missing.iterrows():
    carbs = row['Carbohydrates']
    recipe_id = still_missing.loc[df_idx, 'ID']
    # Check if the units are consistent (i.e. no 'mg')
    if "m" in carbs:
        print("Inconsistent units")
    else:
        amount = float(carbs[:-1])
        units = carbs[-1]
        pdv = round(100 * amount / conversion_ref[5], 2)

        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = amount
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = units
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = pdv

# Check columns with missing data
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    print(f'{col}: {num_na}')

















import numpy as np
points_bins = np.arange(-40, 105, 10)
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)


plt.plot(x_val, y_val)


binned_df['wws_points'].describe()


# Check recipes <40 wws_points
low_points = clean_recipes.loc[clean_recipes['wws_points'] < -40]
low_points


no_oamc = parse_tags(['oamc-freezer-make-ahead'], clean_recipes, 'id')
test_df = clean_recipes.loc[~clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test_df

points_bins = np.arange(-40, 105, 10)
print(len(points_bins))
bins_df = pd.cut(test_df['wws_points'], bins=points_bins)
points_df = test_df.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)

test1_df = clean_recipes.loc[clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test1_df.describe()


# STATISTICAL TESTING and PLOTS
sample_set = clean_recipes['wws_points'].sample(n=500)
sample_set.hist(bins=15)

stats.shapiro(sample_set)


result = stats.anderson(clean_recipes['wws_points'])

print('Statistic: %.3f' % result.statistic)
p = 0
# interpret results
for i in range(len(result.critical_values)):
    slevel, cvalues = result.significance_level[i], result.critical_values[i]
    if result.statistic < result.critical_values[i]:
        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (slevel, cvalues))
    else:
        print('%.3f: %.3f, data does not look normal (reject H0)' % (slevel, cvalues))


# Parse the ingredients column
# Identify how to isolate each tag
tag_string = clean_recipes['ingredients'][1].strip("[]")
tag_string = tag_string.split(', ')
tag_string[0].strip("'")

# Get a list of unique tags
unique_ingredients = []
for string in clean_recipes['ingredients']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_ingredients:
            unique_ingredients.append(tag)

# Display the tags alphabetically
print(f"Number of unique ingredients: {len(unique_ingredients)}")

sorted_ingredients = sorted(unique_ingredients)
sorted_ingredients

for row in range(0, 52):
    sorted_ingredients[row] = sorted_ingredients[row].strip('"')
sorted_ingredients

# open file
with open('ingredients_commas.txt', 'w+') as f:
     
    # write elements of list
    for items in sorted_ingredients:
        f.write('%s, ' %items)
    print("File written successfully")
 
 
# close the file
f.close()
# # for line in sorted_ingredients[0:52]:
# #     print(line.strip('"'))
# sorted = [line.strip('"') for line in unique_ingredients[0:52]]
# unique_ingredients[0:52]


import food
report = food.get_report()
report[32]['Description']


categories = []
for row in range(len(report)):
    categories.append(report[row]['Category'])

new_data = set(categories)
old_data = set(sorted_ingredients)

# if (new_data & old_data):
#     print(new_data & old_data)
if len(new_data.intersection(old_data)) > 0:
    print(new_data.intersection(old_data))
else:
    print("no common elements")
new_data = set([word.lower() for word in list(new_data)])
old_data = set([word.lower() for word in list(old_data)])

if len(new_data.intersection(old_data)) > 0:
    intersect = new_data.intersection(old_data)
    print(len(intersect))
    print(intersect)
else:
    print("no common elements")

# Return dataframe with these common ingredients



# Recipes with negative points are high in protein
negative_points = clean_recipes.loc[clean_recipes['wws_points'] < 0].copy()
print(f'negative_points: {negative_points.shape}')
negative_points.head()


x_values = negative_points['wws_points']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['total_fat_g']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['wws_points']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


recipe_sample = clean_recipes.sample(n=500)
x_values = recipe_sample['wws_points']
y_values = recipe_sample['rating']

fig_test, ax_test = plt.subplots()
ax_test.scatter(x_values, y_values)
linreg_plot(ax_test, x_values, y_values, 80, 80)


# CREATE A BUBBLE PLOT? Bin the WW Smart Points, use the count of each bin as the bubble size (and plot this as the 3rd dimension)



