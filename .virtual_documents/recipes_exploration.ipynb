





# Import libraries
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import scipy.stats as stats

# Import API keys
from alyssa_config import spoonacular_key, rapidapi_key
# from lakna_config import spoonacular_key

# Import functions notebook
%run functions.ipynb





# Import the FOOD.COM datasets as DataFrames.
food_df = pd.read_csv('Resources/RAW_recipes.csv')
interactions_df = pd.read_csv('Resources/RAW_interactions.csv')





# Display the DataFrame
food_df.head()


# Display the DataFrame
interactions_df.head()


# Get the DataFrame dimensions
interactions_shape = interactions_df.shape
food_shape = food_df.shape

# Print findings
hash = f'{8*"#"}'
print(f'{hash} Shape {hash}')
print(f"food_df: {food_shape}")
print(f"interactions_df: {interactions_shape}")

# Get the columns
print(f'\n{hash} Columns {hash}')
print(f'food_df: {food_df.columns}')
print(f'interactions_df: {interactions_df.columns}')

# Get the datatypes
print(f'\n{hash} Data Types {hash}')
print(f'food_df: {food_df.dtypes}')
print(f'\ninteractions_df: {interactions_df.dtypes}')





# Check how many recipes have ratings
unique_ratings = len(interactions_df['recipe_id'].unique())

# Check how many recipes have a '0' rating
zero_rating_df = interactions_df.loc[interactions_df['rating'] == 0]['recipe_id'].unique()

# Drop the rows with a '0' rating
nonzero_df = interactions_df.loc[interactions_df['rating'] != 0]
nonzero_shape = nonzero_df.shape

# Print findings
print(f'Recipes with ratings: {unique_ratings} out of {food_shape[0]}')
print(f'Recipes with a "0" rating: {zero_rating_df.shape[0]}')
print(f'Updated shape: {nonzero_shape}')


# Create a DataFrame with the average ratings per recipe ID
food_ratings = nonzero_df.groupby('recipe_id')['rating'].mean().reset_index()

# Display the DataFrame
food_ratings.head()


# Rename the recipe ID column for merging with food_df
food_ratings = food_ratings.rename(columns={'recipe_id': 'id'})

# Merge the datasets and display updated DataFrame
merged_food = pd.merge(food_df, food_ratings, on='id')

# Confirm the row dimensions, to ensure correct merge
merged_shape = merged_food.shape
print(f'food_ratings rows: {food_ratings.shape[0]}')
print(f'merged_food rows: {merged_shape[0]}')

# Display the DataFrame
merged_food.head()


# Create a histogram of the ratings
merged_food['rating'].hist(bins=20)
plt.title("Food.com Recipes")
plt.xlabel("Rating")
plt.ylabel("Number of Recipes")
plt.show()





# Determine whether there are duplicate recipes by ID
dup_id = len(merged_food['id'].unique())
print(f'Unique recipe IDs: {dup_id} of {merged_shape[0]}')

# Determine whether there are duplicate recipes by name
dup_name = len(merged_food['name'].unique())
print(f'Unique recipe names: {dup_name} of {merged_shape[0]}')

# Get the duplicate names - remove to simplify the dataset
duplicate_names = merged_food.loc[merged_food.duplicated(['name'])]
dupname_shape = duplicate_names.shape
print(f'Number of duplicate rows to remove: {dupname_shape[0]}')

# Create a new DataFrame without the duplicates
updated_food = merged_food.loc[~merged_food.duplicated(['name'])].copy()
updated_shape = updated_food.shape
print(f'updated_food rows: {updated_shape[0]}')


# Display the DataFrame
updated_food.head()





# Identify how to isolate each tag
tag_string = updated_food['tags'][0].strip("[]")
tag_string = tag_string.split(', ')
tag_string[0].strip("'")

# Get a list of unique tags
unique_tags = []
for string in updated_food['tags']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_tags:
            unique_tags.append(tag)

# Display the tags alphabetically
print(f"Number of unique tags: {len(unique_tags)}")
sorted(unique_tags)





# Check the tags for meal types
meal_types = ["breakfast", "lunch", "dinner"]
match_meals = tag_check(meal_types)

# Print the results
print(f'Tags: {match_meals[0]}')
print(f'Not tags: {match_meals[1]}')
print(f'Alternative tags: {match_meals[2]}')

# Update the meal type list
meal_types[2] = "dinner-party"
print(f'Updated meal_types: {meal_types}')


# Get a count of each meal type
meal_dict = parse_tags(meal_types, updated_food, 'id')

# Get the list of all recipes with one meal type
combined_meals = []
for key in meal_dict:
    print(f"{key}: {meal_dict[key]['count']}")
    if (key != 'multiple'):
        combined_meals.append(meal_dict[key]['id_list'])

# Flatten combined_meals
flat_meals = [index for meal_list in combined_meals for index in meal_list]

# Remove multiples to get a list of all recipes with one meal type
unique_meals = [meal for meal in flat_meals if meal not in meal_dict['multiple']['id_list']]

# Print results
print(f'Total recipes with one meal type: {len(unique_meals)}')


# Reduce the dataset to contain recipes with only one meal type
reduced_food = updated_food.loc[updated_food['id'].isin(unique_meals)].copy()
reduced_shape = reduced_food.shape

print(f'reduced_food: {reduced_shape}')
reduced_food.head()


# Create a new column with the meal type
for key in meal_dict:
    for id in meal_dict[key]['id_list']:
        if (key == "dinner-party"):
            key = "dinner"
        reduced_food.loc[reduced_food['id'] == id, 'meal_type'] = key

# Confirm the column exists in reduced_food
reduced_food['meal_type']





# Convert Spoonacular supported cuisines to a list
# input_string = input("List to pass: ")
input_string = """African Asian American British Cajun Caribbean Chinese
    Eastern European European French German Greek Indian Irish Italian
    Japanese Jewish Korean Latin American Mediterranean Mexican Middle Eastern
    Nordic Southern Spanish Thai Vietnamese"""

spoonacular_cuisines = input_string.split(' ')
spoonacular_cuisines = [word.lower() for word in spoonacular_cuisines]
spoonacular_cuisines # DOES NOT ACCOUNT FOR DOUBLE WORD.

# List comprehension to remove double words
double_words = ['eastern', 'european', 'latin', 'american', 'middle']
[spoonacular_cuisines.remove(word) for word in double_words]

# Return the two-word cuisines
spoonacular_cuisines += ['eastern european', 'latin american', 'middle eastern']
spoonacular_cuisines = sorted(spoonacular_cuisines)


# Check the tags for spoonacular cuisines
cuisine_match = tag_check(spoonacular_cuisines)[0]
print(f'Spoonacular Cuisines: {len(spoonacular_cuisines)}, {spoonacular_cuisines}\n')
print(f'Matched cuisines: {len(cuisine_match)}, {cuisine_match}')


# Get a count of each cuisine
cuisine_dict = parse_tags(cuisine_match, reduced_food, 'id')

# Get the list of all recipes with one meal type
combined_cuisines = []
for key in cuisine_dict:
    print(f"{key}: {cuisine_dict[key]['count']}")
    if (key != 'multiple'):
        combined_cuisines.append(cuisine_dict[key]['id_list'])

# Flatten combined_meals to get a list of unique recipes with one meal type
flat_cuisine = list(set([index for cuisine_list in combined_cuisines for index in cuisine_list]))

# Remove multiples to get a list of all recipes with one cuisine type
unique_cuisine = [cuisine for cuisine in flat_cuisine if cuisine not in cuisine_dict['multiple']['id_list']]
print(f'Total recipes with one cuisine type: {len(unique_cuisine)}')


# Reduce the dataset to contain recipes with only one cuisine
one_cuisine = reduced_food.loc[reduced_food['id'].isin(unique_cuisine)].copy()
print(f'reduced_food: {one_cuisine.shape}')
one_cuisine.head()


# Create a new column with the cuisine
for key in cuisine_dict:
    for id in cuisine_dict[key]['id_list']:
        one_cuisine.loc[one_cuisine['id'] == id, 'cuisine'] = key

# Confirm the column exists in reduced_food
one_cuisine['cuisine']





# Identify how to split the nutrition string and convert to float
test_string = one_cuisine['nutrition']

# Remove the square brackets
test_string = test_string[1].strip("[]")

# Split the string to a list
test_string = test_string.split(", ")

# Cast values to float
test_string = [float(value) for value in test_string]
test_string


# Identify how to convert PDV to nutrient quantity
conversion_ref = [65, 50, 2.4, 50, 20, 300]

# Remove 'Calories' from the test string
test_pdv = test_string[1:]

# Convert from PDV to absolute values
abs_values = []
for ref in range(len(conversion_ref)):
    abs_values.append(test_pdv[ref] * conversion_ref[ref] / 100)
abs_values


one_cuisine.head()


# Parse each value in the `nutrition` column
for df_idx, row in one_cuisine.iterrows():
    # Strip and split the string to a list
    values_list = row['nutrition'].strip("[]").split(", ")

    # Allocate each nutritional value to the correct column
    for idx, value in enumerate(values_list):
        if (idx == 0):
            one_cuisine.loc[df_idx, 'Calories'] = float(value)
        elif (idx == 1):
            one_cuisine.loc[df_idx, 'Total Fat (PDV)'] = float(value)
        elif (idx == 2):
            one_cuisine.loc[df_idx, 'Sugar (PDV)'] = float(value)
        elif (idx == 3):
            one_cuisine.loc[df_idx, 'Sodium (PDV)'] = float(value)
        elif (idx == 4):
            one_cuisine.loc[df_idx, 'Protein (PDV)'] = float(value)
        elif (idx == 5):
            one_cuisine.loc[df_idx, 'Saturated Fat (PDV)'] = float(value)
        elif (idx == 6):
            one_cuisine.loc[df_idx, 'Carbohydrates (PDV)'] = float(value)

# Display the DataFrame
one_cuisine.head()


# Convert PDV nutritional values to absolute values (in grams)
merged_subset = one_cuisine[[
    'Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

for df_idx, row in merged_subset.iterrows():
    for col_idx in range(len(row)):
        result = row[col_idx] * conversion_ref[col_idx] / 100
        if (col_idx == 0):
            one_cuisine.loc[df_idx, 'total_fat_g'] = result
        elif (col_idx == 1):
            one_cuisine.loc[df_idx, 'sugar_g'] = result
        elif (col_idx == 2):
            one_cuisine.loc[df_idx, 'sodium_g'] = result
        elif (col_idx == 3):
            one_cuisine.loc[df_idx, 'protein_g'] = result
        elif (col_idx == 4):
            one_cuisine.loc[df_idx, 'sat_fat_g'] = result
        elif (col_idx == 5):
            one_cuisine.loc[df_idx, 'carbs_g'] = result

# Display the DataFrame
print(f'one_cuisine: {one_cuisine.shape}')
one_cuisine.head()





for df_idx, row in one_cuisine.iterrows():
    calories = row.Calories
    sat_fat = row.sat_fat_g
    sugar = row.sugar_g
    protein = row.protein_g
    one_cuisine.loc[df_idx, 'wws_points'] = round((calories * 0.0305) + (sat_fat * 0.275) + (sugar * 1.2) - (protein * 0.98), 0)

# Convert wws_points to integers and display the results
one_cuisine = one_cuisine.astype({'wws_points':'int64'})
one_cuisine.head()








# Check DataFrame datatypes
one_cuisine.dtypes


# Get a list of columns of type int64 and float64, excluding 'id' and 'contributor_id'
calc_cols = one_cuisine.select_dtypes(include=['int64', 'float64']).drop(columns=['id', 'contributor_id'])

# Get the descriptive statistics
calc_stats = calc_cols.describe().T.drop(columns=['count'])

# Calculate the IQR and bounds, then display the descriptive statistics
print(f'Columns with values above "max":{len(add_iqr(calc_stats))}\n{add_iqr(calc_stats)}')
calc_stats = calc_stats.T
calc_stats








# Get upper_bounds for PDV columns
upper_pdv = calc_stats.loc['upper_bounds', ['Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

# Create a dictionary to hold the count and IDs
pdv_dict = {}
for col in upper_pdv.index:
    pdv_dict[col] = dict(count = 0, id_list = [])

# Loop through upper_pdv to get the count and IDs
for idx, value in enumerate(upper_pdv):
    col_name = upper_pdv.index[idx]
    pdv_dict[col_name]['count'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id'].count()
    pdv_dict[col_name]['id_list'] = one_cuisine.loc[one_cuisine[col_name] > value, 'id']

# Get the list of all recipes which exceeds upper bounds for PDV
combined_pdv = []
for key in pdv_dict:
    print(f"{key}: {pdv_dict[key]['count']}")
    combined_pdv.append(pdv_dict[key]['id_list'])

# Flatten combined_pdv
flat_pdv = list(set([index for pdv_list in combined_pdv for index in pdv_list]))
print(f'Total recipes which exceed upper bounds for PDV: {len(flat_pdv)}')


# Explore values outlier values
outside_pdv = one_cuisine.loc[one_cuisine['id'].isin(flat_pdv)]
outside_pdv.sort_values(by='wws_points', ascending=False).head()

# Although PDV correlates to a single serving, the outliers identified are not reasonable.
# This also corresponds to very high WW Smart Points, which is not realistic.
# This could be attributed to errors in inputing serving sizes for the recipe.





# UNHEALTHY RECIPES - PDV well outside the range
x_values = outside_pdv['sat_fat_g']
y_values = outside_pdv['wws_points']
fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
# ax.set_xlabel(


within_pdv = one_cuisine.loc[~one_cuisine['id'].isin(flat_pdv)].copy()
print(f'within_pdv: {within_pdv.shape}')
within_pdv.head()








# Identify recipes with '0' minutes cooking time
zeromin_recipes = within_pdv.loc[within_pdv['minutes'] == 0]
print(f'zeromin_recipes: {zeromin_recipes.shape}')
zeromin_recipes.sort_values(by='minutes', ascending=False)


# Get the upper bounds value from the 'minutes' column
minutes_upper = calc_stats.loc['upper_bounds', 'minutes']

# Identify recipes with very long cooking times
long_recipes = within_pdv.loc[within_pdv['minutes'] > minutes_upper]
print(f'long_recipes: {long_recipes.shape}')
long_recipes.sort_values(by='minutes', ascending=False)





# How many of the long recipes had PDV outliers?
pdv_long = one_cuisine.loc[one_cuisine['id'].isin(flat_pdv)]
pdv_long.sort_values(by='minutes', ascending=False)


pdv_mins = pdv_long.loc[pdv_long['minutes'] > minutes_upper]
print(f'Recipes with PDV outliers: {pdv_long.shape[0]}')
print(f'Long recipes with PDV outliers: {pdv_mins.shape[0]}')


# How many of the long recipes are highly rated?
best_long = long_recipes.loc[long_recipes['rating'] == 5].copy()
best_long['minutes'].describe()





# Remove recipes greater than the calculated upper bounds AND recipes with '0' minutes cooking time
clean_recipes = within_pdv.loc[(within_pdv['minutes'] < minutes_upper) & (within_pdv['minutes'] > 0)].copy()
print(f'clean_recipes: {clean_recipes.shape}')
clean_recipes.head()








# Get the descriptive statistics summary
clean_descript = clean_recipes['wws_points'].describe()
clean_descript


# Create a histogram of the points
clean_recipes['wws_points'].hist(bins=20)
plt.title("Food.com Recipes")
plt.xlabel("WW Smart Points")
plt.ylabel("Number of Recipes")
plt.show()


# Check for Gaussian distribution - using Shapiro-Wilk test
# Note: Shapiro-Wilk is sensitive to sample size
sample_set = clean_recipes['wws_points'].sample(n=500)
sample_set.hist(bins=15)
plt.title("Food.com Recipes")
plt.xlabel("Weight Watchers (WW) Smart Points")
plt.ylabel("Number of Recipes")

stats.shapiro(sample_set)
# A very high W-statistic suggests a good fit to a normal distribution.
# A near-zero p-value, for a Shapiro-Wilk test, rejects the null hypothesis, meaning NOT a normal distribution.





# Get clean_recipes['wws_points'] outliers
points_outliers = add_iqr(clean_descript)
points_outliers


clean_recipes1 = clean_recipes.loc[clean_recipes['wws_points'] < points_outliers.upper_bounds]
clean_recipes1['wws_points'].hist(bins=15)

sample_set1 = clean_recipes1['wws_points'].sample(n=500)
stats.shapiro(sample_set1)
# Removing outliers creates in a higher W-statistic, however the p-value is still near-zero.





pdv_cut = clean_recipes[['rating', 'Calories', 'Total Fat (PDV)', 'Sugar (PDV)', 'Sodium (PDV)', 'Protein (PDV)', 'Saturated Fat (PDV)', 'Carbohydrates (PDV)']]

corr_matrix = round(pdv_cut.corr(), 2)
corr_matrix

corr_matrix.style.background_gradient(cmap='Blues')








# Isolate 'breakfast, lunch, dinner' from clean_recipes
meal_df = clean_recipes.groupby(['meal_type']).mean(numeric_only=True)
meal_df = meal_df.drop(columns=['id', 'contributor_id'])
meal_df


# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes['wws_points'].describe().min()
max_points = clean_recipes['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points+16, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
points_labels = [f'{points_bins[i]} to {points_bins[i+1]}' for i in range(len(points_bins)-1)]
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins, labels=points_labels)

# Set the index and its name
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges', 'meal_type']).median(numeric_only=True) # originally mean
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)

## RESEARCH HOW TO PLOT MULTI-INDEX DATAFRAME


index = pd.MultiIndex.from_tuples([('A', 'Category1'), ('A', 'Category2'), ('B', 'Category1'), ('B', 'Category2')], names=['Group', 'Category'])
data = {'Value1': np.random.rand(4) * 100, 'Value2': np.random.rand(4) * 100}

df = pd.DataFrame(data, index=index)
df


# Extract data for plotting
x = binned_df.index.get_level_values('WWS Point Ranges')
y = binned_df.index.get_level_values('meal_type')
size = binned_df['protein_g']  # Size of the bubbles
color = binned_df['rating']  # Color of the bubbles

# Create the bubble chart
fig, ax = plt.subplots(figsize=(8, 6))
scatter = ax.scatter(x, y, s=size * 10, c=color, cmap='viridis', alpha=0.7)

# Add labels and a colorbar
ax.set_xlabel('WWS Points')
plt.xticks(rotation=45)
ax.set_ylabel('Category')
ax.set_title('Bubble Chart')
cbar = plt.colorbar(scatter)
cbar.set_label('Value2')

plt.show()


# THOUGHTS: Group by ratings instead? Rather than points? More robust and less iffy that way?
# ANSWER: Tried it, doesn't really work!


updated_types = clean_recipes['meal_type'].unique()
print(updated_types)

# Breakfast
breakfast = clean_recipes.loc[clean_recipes['meal_type'] == "breakfast"].copy()
print(f'breakfast: {breakfast.shape}')

# Lunch
lunch = clean_recipes.loc[clean_recipes['meal_type'] == "lunch"].copy()
print(f'lunch: {lunch.shape}')

# Dinner
dinner = clean_recipes.loc[clean_recipes['meal_type'] == "dinner"].copy()
print(f'dinner: {dinner.shape}')


fig, axs = plt.subplots(3)
x_col = 'wws_points'
y_col = 'rating'

df_list = [breakfast, lunch, dinner]

for idx, val in enumerate(df_list):
    axs[idx].scatter(val[x_col], val[y_col])


# Bin and count to get percentages?





# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes['wws_points'].describe().min()
max_points = clean_recipes['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)

# Set the index and its name
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).median(numeric_only=True) # originally mean
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)





# Get the DataFrame for 5-star ratings
top_rated = clean_recipes.loc[clean_recipes['rating'] >= 5].copy()
print(f'top_rated: {top_rated.shape}')


# Get the DataFrame for all other ratings
bottom_rated = clean_recipes.loc[clean_recipes['rating'] < 5].copy()
print(f'bottom_rated: {bottom_rated.shape}')


# Bin by `wws_points` then groupby each meal type
min_points = bottom_rated['wws_points'].describe().min()
max_points = bottom_rated['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(bottom_rated['wws_points'], bins=points_bins)

# Set the index and its name
points_df = bottom_rated.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['protein_g']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['sugar_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Protein (g)")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 150, 200)


# Bin by `wws_points` then groupby each meal type
min_points = top_rated['wws_points'].describe().min()
max_points = top_rated['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(top_rated['wws_points'], bins=points_bins)

# Set the index and its name
points_df = top_rated.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True) # originally mean
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['protein_g']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['sugar_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("sat_fat_g")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)





low_points = clean_recipes.loc[clean_recipes['wws_points'] < -40].copy()
low_points.describe()
low_points.shape


# Bin by `wws_points` then groupby each meal type
min_points = low_points['wws_points'].describe().min()
max_points = low_points['wws_points'].describe().max()

x_val = low_points['wws_points']
y_val = low_points['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Protein (g)")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 150, 200)


low_points['id']


low_points


# Remove outliers?
clean_recipes1 = clean_recipes.loc[~clean_recipes['id'].isin(low_points['id'])]

# Bin by `wws_points` then groupby each meal type
min_points = clean_recipes1['wws_points'].describe().min()
max_points = clean_recipes1['wws_points'].describe().max()

# Create bins
points_bins = np.arange(min_points, max_points, 16) # CHANGE BETWEEN 10 and 16, and look at PLOT!
bins_df = pd.cut(clean_recipes1['wws_points'], bins=points_bins)

# Set the index and its name
points_df = clean_recipes1.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"

# Create grouped DataFrame by numeric means
binned_df = points_df.groupby(['WWS Point Ranges']).median(numeric_only=True) # originally mean
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*15, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)














# Uncomment below to run [API REQUEST - Complex Search]
# complex_search()

# Import and display complex search results
recipes_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_0.csv')
print(f'recipes_df: {recipes_df.shape}')
recipes_df.head()

# Uncomment below to run [API REQUEST - Recipe Information]
# recipe_info('Resources/02_raw_data/info_master_0.csv')

# Uncomment below to run [API ACTIVE - Check Response]
# check_response()


# Import and display recipe information results
info_df = pd.read_csv(f'Resources/02_raw_data/info_master_0.csv')

# Display the DataFrame and its columns
print(info_df.columns)
info_df.head()


# Parse the API response - all relevant columns, and account for different units

# Uncomment below to run [API ACTIVE - Parse Response]
# parse_response('Resources/03_simplified_data/initial_nutrition_0.csv')

# Import and display recipe information results
simple_df = pd.read_csv(f'Resources/03_simplified_data/initial_nutrition_0.csv')

# Display the DataFrame and its columns
print(simple_df.columns)
simple_df.head()


# Check the units columns
units_columns = ['Calories (Unit)', 'Saturated Fat (Unit)', 'Sugar (Unit)', 'Sodium (Unit)', 'Protein (Unit)']
for col in units_columns:
    check = simple_df[col].unique()
    print(f"Unit check: {col} {check}")

# Account for the differences when calculating points





# Uncomment below to run [API REQUEST - Random Recipe]
# random_recipe('Resources/01_recipe_IDs/initial_recipes_4.csv')

# Import and display random recipes results
random_df = pd.read_csv(f'Resources/01_recipe_IDs/initial_recipes_4.csv')
random_df.head()


# # Uncomment block to run [API REQUEST - Recipe Information]
# recipe_info('Resources/02_raw_data/info_master_4.csv')

# # Uncomment block to run [API ACTIVE - Parse Response]
# parse_response('Resources/03_simplified_data/initial_nutrition_4.csv')





# # Uncomment block to run [API REQUEST - Random Recipe and Nutrition by ID]
# recipe_info('Resources/02_raw_data/info_master_5.csv')

# # Uncomment block to run [API REQUEST - Nutrition by ID]
# nutrition_id('Resources/02_raw_data/info_master_5.csv')

# # Uncomment block to parse the response (added carbohydrates)
# parse_metadata(
#     'Resources/01_recipe_IDs/initial_recipes_5.csv',
#     'Resources/03_simplified_data/initial_nutrition_5.csv')





# Uncomment the line below to run Method 1
# spoonacular_v1(type=2)

# Uncomment the line below to run the API request
# spoonacular_v2()





# Get list of subdirectories in the Resources folder
from pathlib import Path
subdir_list = []
for path in Path('Resources').iterdir():
    # Ignore `.ipynb_checkpoints`
    if (path == Path('Resources/.ipynb_checkpoints')):
        continue
    elif path.is_dir():
        subdir_list.append(path)

# Loop over folders without simplified data
raw_ids = []
raw_files = []
for dir in subdir_list[0:2]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory, ignore `.ipynb_checkpoints`
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints'):
            filename = f'{dir}/{file.name}'
            csv_df = pd.read_csv(filename)
            try:
                raw_ids.append(csv_df['id'])
                raw_files.append(filename)
            except:
                # If no 'id' column, move on to the next csv
                continue

# Get a unique list of recipe IDs
raw_list = [id for row in raw_ids for id in row]
unique_raw = list(set(raw_list))

# Remove NaN values from unique_raw
clean_raw = []
for value in unique_raw:
    try:
        clean_raw.append(int(value))
    except:
        # Value cannot be cast to int
        continue
print(f'Unique out of raw data: {len(clean_raw)} of {len(raw_list)}')

# Loop over folders with simplified data
simple_ids = []
simple_files = []
for dir in subdir_list[2:]:
    # Read each file in the directory
    for file in os.scandir(dir):
        # Check if the file is NOT a directory, ignore `.ipynb_checkpoints`
        if os.path.isfile(file) & (file.name != '.ipynb_checkpoints') & (file.name.split('.')[-1] == 'csv'):
                filename = f'{dir}/{file.name}'
                csv_df = pd.read_csv(filename)
                try:
                    try:
                        simple_ids.append(csv_df['id'])
                    except:
                        simple_ids.append(csv_df['ID'])
                    simple_files.append(filename)
                except:
                    # If no 'id' column, move on to the next csv
                    continue

# Get a unique list of recipe IDs
simple_list = [id for row in simple_ids for id in row]
unique_simple = list(set(simple_list))
print(f'Unique out of simplified data: {len(unique_simple)} of {len(simple_list)}')

# Identify the intersection between the unique raw and unique simplified data
intersect_ids = []
missing_ids = set(clean_raw).difference(set(unique_simple))
if set(clean_raw) & set(unique_simple):
    intersect_ids.append(set(clean_raw) & set(unique_simple))
print(f'Total intersection: {len(intersect_ids[0])}')
print(f'Total missing: {len(missing_ids)}')


# If there are IDs with missing information, get the data
if len(missing_ids) > 0:
    spoonacular_v3(missing_ids)





# Import one file from 03_simplified_data as a reference
simple_ref = pd.read_csv('Resources/03_simplified_data/initial_nutrition_7.csv').columns
simple_ref


# Import one file from 04_complex_test as a reference
complex_ref = pd.read_csv('Resources/04_complex_test/simplified_data_0.csv').columns
complex_ref


# Read each simplified data file and create a DataFrame
df_list = []
for file in simple_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
simplified_df = pd.concat(df_list, ignore_index=True)

# Identify duplicate IDs
duplicate_IDs = simplified_df.loc[simplified_df.duplicated(['ID'])]
print(f'Number of duplicate rows to remove: {duplicate_IDs.shape[0]}')

# Create a new DataFrame without the duplicates
simplified_df = simplified_df.loc[~simplified_df.duplicated(['ID'])].copy()

# Display the DataFrame, columns, and shape
print(f'simplified_df: {simplified_df.shape}')
print(simplified_df.columns)
simplified_df.head()


# Read each raw data file and create a DataFrame
df_list = []
for file in raw_files:
    one_df = pd.read_csv(file)
    df_list.append(one_df)
raw_df = pd.concat(df_list, ignore_index=True)

# Drop rows with missing IDs
raw_df = raw_df.loc[~raw_df['id'].isna()]

# Reduce DataFrame to unique IDs
raw_df = raw_df.loc[~raw_df.duplicated(['id'])].copy()

# Display the DataFrame, columns, and shape
print(f'raw_df: {raw_df.shape}')
print(raw_df.columns)
raw_df.head()


# Minimum columns required
min_cols = ['ID', 'Name', 'Likes', 'Meal Type', 'Cuisines', 'N_ingredients',
       'WW Smart Points', 'Cooking Minutes', 'Calories (Amount)', 'Calories (Unit)',
        'Calories (% of Daily Needs)', 'Saturated Fat (Amount)', 'Saturated Fat (Unit)',
       'Saturated Fat (% of Daily Needs)', 'Sugar (Amount)', 'Sugar (Unit)',
       'Sugar (% of Daily Needs)', 'Sodium (Amount)', 'Sodium (Unit)',
       'Sodium (% of Daily Needs)', 'Protein (Amount)', 'Protein (Unit)',
       'Protein (% of Daily Needs)', 'Carbs (Amount)', 'Carbs (Unit)',
       'Carbs (% of Daily Needs)']

# Identify columns with missing data
missing_cols = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')





# Get the list of recipe IDs with missing data
missing_recipes = check_na['ID']

# Check if the missing data have rows in raw_df
find_missing = raw_df.loc[raw_df['id'].isin(missing_recipes)]

# Check if there is a nutrition column for the missing data
find_nutrition = raw_df.loc[~raw_df['nutrition'].isna()]
print(f'Missing recipes in raw_df with a nutrition column: {len(find_nutrition)}')


# Test get_nutrition() function
test_nutrition = get_nutrition(raw_df['nutrition'][0], 'Carbohydrates')
test_nutrition


# Populate simplified_df with the missing data
for df_idx, row in find_nutrition.iterrows():
    result = get_nutrition(row['nutrition'], 'Carbohydrates')
    recipe_id = find_nutrition.loc[df_idx, 'id']
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = result[0]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = result[1]
    simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = result[2]

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')

# Flatten list of missing IDs
missing_list = [id for id_list in missing_ids for id in id_list]


# Identify the recipes with still missing values
still_missing = simplified_df.loc[simplified_df['ID'].isin(missing_list)]
check_column = still_missing.loc[still_missing['Carbohydrates'].isna()]

# Display the DataFrame
print(f'Carbohydrates column with missing values: {len(check_column)}')
still_missing.head() # Data is in 'Carbohydrates' column


# Drop null values from the carbohydrates column
still_missing = still_missing.dropna(subset='Carbohydrates', how="any")

# Add carbohydrates columns
for df_idx, row in still_missing.iterrows():
    carbs = row['Carbohydrates']
    recipe_id = still_missing.loc[df_idx, 'ID']
    # Check if the units are consistent (i.e. no 'mg')
    if "m" in carbs:
        print("Inconsistent units")
    else:
        amount = float(carbs[:-1])
        units = carbs[-1]
        pdv = round(100 * amount / conversion_ref[5], 2)

        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Amount)'] = amount
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (Unit)'] = units
        simplified_df.loc[simplified_df['ID'] == recipe_id, 'Carbs (% of Daily Needs)'] = pdv

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')

# Flatten list of missing IDs
missing_list = [id for id_list in missing_ids for id in id_list]


# Identify the recipes with still missing values
still_missing = simplified_df.loc[simplified_df['ID'].isin(missing_list)]
check_column = still_missing.loc[still_missing['Cooking Minutes'].isna()]

# Display the DataFrame
print(f'Cooking Minutes column with missing values: {len(check_column)}')
still_missing.head() # Data is in 'readyInMinutes' column

# Drop the 'Prep Minutes' column
simplified_df = simplified_df.drop(columns=['Prep Minutes'])
simplified_df.columns


# Add cooking minutes column
for df_idx, row in still_missing.iterrows():
    recipe_id = still_missing.loc[df_idx, 'ID']
    mins = raw_df.loc[raw_df['id'] == recipe_id, 'readyInMinutes']
    simplified_df['Cooking Minutes'] = float(mins)

# Identify columns with missing data
missing_cols = []
missing_ids = []
for col in min_cols:
    check_na = simplified_df.loc[simplified_df[col].isna()]
    num_na = len(check_na)
    if (num_na > 0):
        missing_cols.append(col)
        missing_ids.append(check_na['ID'])
    print(f'{col}: {num_na}')
print(f'\nColumns with null values: {missing_cols}')








# Get a list of columns of type int64 and float64, excluding 'id' and 'contributor_id'
calc_cols = simplified_df.select_dtypes(include=['int64', 'float64']).drop(columns=['ID'])

# Calculate IQR and bounds, then display the descriptive statistics
calc_stats = calc_cols.describe().T.drop(columns=['count'])
print(f'Columns with values above "max":{len(add_iqr(calc_stats))}\n{add_iqr(calc_stats)}')
calc_stats = calc_stats.T
calc_stats





# Create a histogram of the Likes column
simplified_df['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()


# Identify recipes with '0' likes
zero_likes = simplified_df.loc[simplified_df['Likes'] == 0].copy()
print(f'zero_likes: {zero_likes.shape}')





# Get the upper bounds for Likes
upper_likes = calc_stats.loc['upper_bounds', 'Likes']
likes_outliers = simplified_df.loc[simplified_df['Likes'] > upper_likes].copy()

# Display the count of outlier recipes
print(f"Number of outliers: {likes_outliers['Likes'].count()}")

# Create a histogram to understand distribution
likes_outliers['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes - Likes Outliers")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()


# Get the descriptive statistics for the outliers
outlier_stats = likes_outliers['Likes'].describe()
outlier_stats = add_iqr(outlier_stats)
print(outlier_stats)

# Get the DataFrame for outliers of outliers
outliers_squared = likes_outliers.loc[likes_outliers['Likes'] > outlier_stats['upper_bounds']]
print(f'outliers_squared: {outliers_squared.shape}')
outliers_squared.head()


# Plot the relationship between outliers of outliers' WW Smart Points vs Likes
x_values = outliers_squared['WW Smart Points']
y_values = outliers_squared['Calories (Amount)']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values, s=outliers_squared['Likes'], alpha=0.3)
linreg_plot(ax, x_values, y_values, 100, 100)
plt.show()


# Get Likes within the limits
likes_within = simplified_df.loc[simplified_df['Likes'] < upper_likes]

# Print the count of recipes within limits
print(f"Number of recipes within limits: {likes_within['Likes'].count()}")

# Create a histogram to understand distribution
likes_within['Likes'].hist(bins=10)
plt.title("Spoonacular Recipes - Likes within Limits")
plt.xlabel("Aggregate Likes")
plt.ylabel("Number of Recipes")
plt.show()








# Food.com columns
clean_recipes.columns


# Current Spoonacular data columns
simplified_df.columns














import numpy as np
points_bins = np.arange(-50, 105, 10)
bins_df = pd.cut(clean_recipes['wws_points'], bins=points_bins)
points_df = clean_recipes.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df


x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
ax.set_xlabel("WW Smart Points")
ax.set_ylabel("Rating")
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)


plt.plot(x_val, y_val)


binned_df['wws_points'].describe()


# Check recipes <40 wws_points
low_points = clean_recipes.loc[clean_recipes['wws_points'] < -40]
low_points


no_oamc = parse_tags(['oamc-freezer-make-ahead'], clean_recipes, 'id')
test_df = clean_recipes.loc[~clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test_df

points_bins = np.arange(-40, 105, 10)
print(len(points_bins))
bins_df = pd.cut(test_df['wws_points'], bins=points_bins)
points_df = test_df.set_index(bins_df)
points_df.index.name = "WWS Point Ranges"
points_df

binned_df = points_df.groupby(['WWS Point Ranges']).mean(numeric_only=True)
binned_df

x_val = binned_df['wws_points']
y_val = binned_df['rating']
fig, ax = plt.subplots()
ax.scatter(x_val, y_val, s=binned_df['protein_g']*10, alpha=0.3)
# ax.plot(x_val, y_val)
linreg_plot(ax, x_val, y_val, 100, 100)

test1_df = clean_recipes.loc[clean_recipes['id'].isin(no_oamc['oamc-freezer-make-ahead']['id_list'])]
test1_df.describe()


# STATISTICAL TESTING and PLOTS
sample_set = clean_recipes['wws_points'].sample(n=500)
sample_set.hist(bins=15)

stats.shapiro(sample_set)


result = stats.anderson(clean_recipes['wws_points'])

print('Statistic: %.3f' % result.statistic)
p = 0
# interpret results
for i in range(len(result.critical_values)):
    slevel, cvalues = result.significance_level[i], result.critical_values[i]
    if result.statistic < result.critical_values[i]:
        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (slevel, cvalues))
    else:
        print('%.3f: %.3f, data does not look normal (reject H0)' % (slevel, cvalues))


# Parse the ingredients column
# Identify how to isolate each tag
tag_string = clean_recipes['ingredients'][1].strip("[]")
tag_string = tag_string.split(', ')
tag_string[0].strip("'")

# Get a list of unique tags
unique_ingredients = []
for string in clean_recipes['ingredients']:
    # Strip and split the string to a list
    tag_list = string.strip("[]").split(', ')
    for word_idx in range(len(tag_list)):
        # Get the tag
        tag = tag_list[word_idx].strip("'")
        if tag not in unique_ingredients:
            unique_ingredients.append(tag)

# Display the tags alphabetically
print(f"Number of unique ingredients: {len(unique_ingredients)}")

sorted_ingredients = sorted(unique_ingredients)
sorted_ingredients

for row in range(0, 52):
    sorted_ingredients[row] = sorted_ingredients[row].strip('"')
sorted_ingredients

# open file
with open('ingredients_commas.txt', 'w+') as f:
     
    # write elements of list
    for items in sorted_ingredients:
        f.write('%s, ' %items)
    print("File written successfully")
 
 
# close the file
f.close()
# # for line in sorted_ingredients[0:52]:
# #     print(line.strip('"'))
# sorted = [line.strip('"') for line in unique_ingredients[0:52]]
# unique_ingredients[0:52]


import food
report = food.get_report()
report[32]['Description']


categories = []
for row in range(len(report)):
    categories.append(report[row]['Category'])

new_data = set(categories)
old_data = set(sorted_ingredients)

# if (new_data & old_data):
#     print(new_data & old_data)
if len(new_data.intersection(old_data)) > 0:
    print(new_data.intersection(old_data))
else:
    print("no common elements")
new_data = set([word.lower() for word in list(new_data)])
old_data = set([word.lower() for word in list(old_data)])

if len(new_data.intersection(old_data)) > 0:
    intersect = new_data.intersection(old_data)
    print(len(intersect))
    print(intersect)
else:
    print("no common elements")

# Return dataframe with these common ingredients



# Recipes with negative points are high in protein
negative_points = clean_recipes.loc[clean_recipes['wws_points'] < 0].copy()
print(f'negative_points: {negative_points.shape}')
negative_points.head()


x_values = negative_points['wws_points']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['total_fat_g']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['protein_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['wws_points']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


x_values = negative_points['Calories']
y_values = negative_points['carbs_g']

fig, ax = plt.subplots()
ax.scatter(x_values, y_values)
linreg_plot(ax, x_values, y_values, 80, 80)


recipe_sample = clean_recipes.sample(n=500)
x_values = recipe_sample['wws_points']
y_values = recipe_sample['rating']

fig_test, ax_test = plt.subplots()
ax_test.scatter(x_values, y_values)
linreg_plot(ax_test, x_values, y_values, 80, 80)


# CREATE A BUBBLE PLOT? Bin the WW Smart Points, use the count of each bin as the bubble size (and plot this as the 3rd dimension)



